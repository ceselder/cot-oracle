"""
Chunked ConvQA â€” Queries about the suffix (future reasoning) of a truncated CoT.

The oracle sees activations from the prefix only (up to the truncation point).
The target answer was generated by Gemini seeing the full CoT (gt_response).
A black-box monitor seeing only the prefix text gets ~75% correct; the oracle
should do much better by reading latent representations of future reasoning.

Data source: mats-10-sprint-cs-jb/cot-oracle-convqa-chunked (HuggingFace).
"""

import random
import re

from transformers import AutoTokenizer


HF_DATASET = "mats-10-sprint-cs-jb/cot-oracle-convqa-chunked"


def load_cot_chunked_convqa_data(
    corpus_path: str,
    tokenizer: AutoTokenizer,
    model_name: str,
    num_examples: int = 10000,
    stride: int | str = None,
    n_prompt_positions: int = 5,
    seed: int = 42,
    **_kwargs,
) -> list[dict]:
    """Load chunked ConvQA training data from HuggingFace.

    Each example: oracle sees activations from prefix of CoT (up to chunk_index),
    answers a query about the suffix. Target is Gemini's ground-truth response.
    """
    from cot_utils import get_cot_positions, get_injection_layers

    random.seed(seed)
    LAYERS = get_injection_layers(model_name)

    # Load train split from HF
    rows = _load_from_hf(split="train")
    random.shuffle(rows)
    print(f"  Loaded {len(rows)} chunked ConvQA rows from HF (train split)")

    def _get_prompt_positions(formatted_len: int, n: int = 5) -> list[int]:
        if formatted_len < n:
            return list(range(formatted_len))
        step = formatted_len / (n + 1)
        return [int(step * (i + 1)) for i in range(n)]

    datapoints = []
    skipped = 0

    for row in rows:
        if len(datapoints) >= num_examples:
            break

        question = row["question"]
        cot_text = row["cot_text"]
        query = row["prompt"]
        target_response = row["target_response"]
        chunk_index = row["chunk_index"]
        num_chunks = row["num_chunks"]

        if not target_response or not target_response.strip():
            skipped += 1
            continue

        # Tokenize question + full CoT to get token positions
        messages = [{"role": "user", "content": question}]
        formatted = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True, enable_thinking=True,
        )

        # Build prefix text (sentences 0..chunk_index)
        # Split CoT into sentences to find truncation point
        sentences = re.split(r'(?<=[.!?])\s+', cot_text.strip())
        sentences = [s.strip() for s in sentences if s.strip()]
        if chunk_index + 1 >= len(sentences):
            skipped += 1
            continue
        prefix_text = " ".join(sentences[:chunk_index + 1])

        # Tokenize full text up to end of prefix to get the truncation token position
        full_prefix = formatted + prefix_text
        full_prefix_ids = tokenizer(full_prefix, add_special_tokens=False)["input_ids"]
        prompt_ids = tokenizer(formatted, add_special_tokens=False)["input_ids"]
        prompt_len = len(prompt_ids)
        trunc_pos = len(full_prefix_ids)

        if trunc_pos - prompt_len < 10:
            skipped += 1
            continue

        # Get stride positions up to truncation point
        positions = get_cot_positions(
            prompt_len, trunc_pos,
            stride=stride, tokenizer=tokenizer, input_ids=full_prefix_ids,
        )
        if len(positions) < 2:
            skipped += 1
            continue

        prompt_positions = _get_prompt_positions(prompt_len, n_prompt_positions)
        combined = prompt_positions + positions
        context_positions = combined * len(LAYERS)
        num_positions = len(context_positions)

        max_pos = max(positions)
        context_slice = full_prefix_ids[:max_pos + 1]

        layers_str = ", ".join(str(l) for l in LAYERS)
        oracle_prompt = f"Activations from {num_positions} positions across layers {layers_str}. {query}"

        datapoints.append({
            "datapoint_type": "cot_chunked_convqa",
            "prompt": oracle_prompt,
            "target_response": target_response,
            "layer": LAYERS[0],
            "layers": LAYERS,
            "num_positions": num_positions,
            "context_input_ids": context_slice,
            "context_positions": context_positions,
        })

    print(f"  Generated {len(datapoints)} chunked_convqa examples ({skipped} skipped)")
    return datapoints[:num_examples]


def _load_from_hf(split: str = "train") -> list[dict]:
    """Load chunked ConvQA dataset from HuggingFace."""
    import importlib
    hf_datasets = importlib.import_module("datasets")

    print(f"  Downloading chunked ConvQA from {HF_DATASET} (split={split})...")
    ds = hf_datasets.load_dataset(HF_DATASET, split=split)
    rows = [dict(row) for row in ds]
    print(f"  Downloaded {len(rows)} rows")
    return rows
