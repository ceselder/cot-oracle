"""
Task 7: CoT Summary Prediction (~15K examples)

Given activations at sentence boundaries, predict a concise faithful summary
of the chain of thought. Summaries are pre-generated by Gemini 2.5 Flash Lite.

This teaches the oracle to read activations and produce a semantic description
of what the model actually computed â€” the foundation for unfaithfulness detection.
"""

import json
import random

from tqdm.auto import tqdm
from transformers import AutoTokenizer


def load_cot_summary_data(
    corpus_path: str,
    summaries_path: str,
    tokenizer: AutoTokenizer,
    model_name: str,
    layer_percents: list[int],
    num_examples: int = 15000,
    max_sentences: int = 15,
    seed: int = 42,
    corpus_entries: list[dict] | None = None,
) -> list[dict]:
    """
    Generate CoT summary prediction training data.

    Each example: activations at sentence boundaries -> faithful summary text.
    Summaries come from summaries.jsonl (LLM-generated).
    """
    from cot_utils import layer_percent_to_layer

    random.seed(seed)

    if corpus_entries is not None:
        corpus = {e["id"]: e for e in corpus_entries}
    else:
        corpus = {}
        with open(corpus_path) as f:
            for line in f:
                if line.strip():
                    entry = json.loads(line)
                    corpus[entry["id"]] = entry

    summaries = {}
    with open(summaries_path) as f:
        for line in f:
            if line.strip():
                s = json.loads(line)
                if not s["summary"].startswith("[FAILED"):
                    summaries[s["id"]] = s["summary"]

    layers = [layer_percent_to_layer(model_name, lp) for lp in layer_percents]

    cached = []
    matched_entries = [(corpus[eid], summaries[eid]) for eid in corpus if eid in summaries and len(corpus[eid].get("boundary_positions", [])) >= 2]
    print(f"  Matched corpus+summary entries: {len(matched_entries)}")

    assert matched_entries, "No matched entries found. Check summaries_path."

    for entry, summary in tqdm(matched_entries, desc="  summary: tokenizing corpus", leave=False):
        if "_ctx_ids" in entry:
            context_ids = entry["_ctx_ids"]
        else:
            messages = [{"role": "user", "content": entry["question"]}]
            formatted = tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True, enable_thinking=True,
            )
            cot_text = entry["cot_response"]
            think_end = cot_text.find("</think>")
            if think_end != -1:
                cot_text = cot_text[:think_end]
            context_ids = tokenizer(formatted + cot_text, add_special_tokens=False)["input_ids"]

        positions = [p for p in entry["boundary_positions"][:max_sentences] if p < len(context_ids)]
        if len(positions) < 2:
            continue
        cached.append((context_ids, positions, summary))

    datapoints = []
    pbar = tqdm(total=num_examples, desc="  summary: sampling", leave=False)

    while len(datapoints) < num_examples:
        context_ids, positions, summary = random.choice(cached)

        N = len(positions)

        prompt = (
            f"Activations from {N} sentence boundaries. "
            f"Generate a concise summary of the thought process."
        )

        datapoints.append({
            "datapoint_type": "cot_summary",
            "prompt": prompt,
            "target_response": summary,
            "layers": layers,
            "num_positions": N,
            "context_input_ids": list(context_ids),
            "context_positions": list(positions),
        })
        pbar.update(1)

    pbar.close()
    return datapoints[:num_examples]
