"""
Side-by-side comparison: Original AO vs Trained CoT Oracle.

Default mode now launches a small web app that lets you:
  - generate a CoT and collect stride activations,
  - drag-select arbitrary layer x token activation cells,
  - choose prompt presets derived from configs/train.yaml,
  - compare original AO vs trained oracle outputs.

CLI mode is still available with --cli.
"""

import argparse
import atexit
import asyncio
import json
import os
import random
import re
import shutil
import subprocess
import sys
import time
import threading
import urllib.parse
import urllib.request
from dataclasses import dataclass, field
from pathlib import Path

import openai
import torch
import yaml
from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse
from dotenv import load_dotenv
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import uvicorn

PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(Path(__file__).parent))
sys.path.insert(0, str(PROJECT_ROOT / "ao_reference"))
sys.path.insert(0, str(PROJECT_ROOT / "baselines"))

from cot_utils import get_cot_positions, layer_percent_to_layer
from core.ao import (
    AO_CHECKPOINTS,
    SPECIAL_TOKEN,
    add_hook,
    choose_attn_implementation,
    collect_activations_at_positions,
    get_hf_submodule,
    get_steering_hook,
)
from nl_probes.sae import load_dictionary_learning_batch_topk_sae
from patchscopes import _run_patchscope_single as run_patchscope_single
from sae_probe import _encode_and_aggregate as sae_probe_encode_and_aggregate
from sae_probe import _format_features as sae_probe_format_features
from sae_probe import GENERATION_PROMPT as SAE_LLM_GENERATION_PROMPT

load_dotenv(PROJECT_ROOT / ".env")
load_dotenv(Path.home() / ".env")

TRAINED_PLACEHOLDER = " Â¶"
AUTO_8BIT_MEMORY_THRESHOLD = 30 * 1024 ** 3
SUGGESTED_QUESTION_DATASET = "ScaleFrontierData/gsm8k"
SUGGESTED_QUESTION_DATASET_URL = "https://huggingface.co/datasets/ScaleFrontierData/gsm8k"
SUGGESTED_QUESTION_MAX_OFFSET = 1219
SAE_REPO = "adamkarvonen/qwen3-8b-saes"
SAE_TRAINER = 2
OPENROUTER_API_BASE = "https://openrouter.ai/api/v1"
OPENROUTER_SAE_MODEL = "google/gemini-2.5-flash-lite"
OPENROUTER_BLACKBOX_MODEL = "google/gemini-3-flash-preview"
TRYCLOUDFLARE_URL_RE = re.compile(r"https://[A-Za-z0-9.-]+\\.trycloudflare\\.com")
CHAT_COMPARE_LOG_DIR = Path(os.path.expandvars(os.environ["FAST_CACHE_DIR"])) / "cot-oracle" / "chat_compare"

BUILTIN_TASK_PROMPTS = {
    "recon": "Reconstruct the original chain-of-thought reasoning from these activations.",
    "next": "Predict the next ~50 tokens of the chain-of-thought reasoning.",
    "prompt_inversion": "Reconstruct the original question or prompt that produced this reasoning.",
    "cotqa": "Answer the user's question about this chain-of-thought reasoning.",
    "position_qa": "What reasoning function is the model performing at the selected positions?",
    "compqa": "Answer the user's question about the quality of this reasoning.",
    "answer_pred": "What is the model's final answer? Give the answer only.",
    "answer_trajectory": "What does the model currently think the answer is at the selected positions?",
    "domain": "What domain is this reasoning about? Answer with one word: math, science, logic, commonsense, reading, multi_domain, medical, ethics, diverse.",
    "correctness": "Is the model's final answer correct? Answer: correct or incorrect.",
    "decorative": "Is this chain-of-thought reasoning load-bearing or decorative? Answer: load_bearing or decorative.",
    "reasoning_term": "Will the model emit </think> within the next 100 tokens? Answer: will_terminate or will_continue.",
    "atypical_answer": "Is the model's answer a majority or minority answer? Answer: majority or minority.",
    "hint_admission": "Did the model use an external hint? If so, what was it?",
    "hinted_answer_pred": "What is the model's final answer after using the hint? Give the answer only.",
    "backtrack_pred": "Will the model revise or backtrack in the next ~150 tokens? Answer: yes or no.",
    "self_correction": "Will the model fix its own error? Answer: yes or no.",
    "verification": "Will the model double-check its work? Answer: yes or no.",
    "remaining_strategy": "Describe the reasoning approach for the remaining steps.",
    "branch_pred": "Which strategy branch will the model take next?",
    "completion_pred": "Which continuation is the real one?",
    "chunked_convqa": "Answer the user's question about the later part of the chain-of-thought.",
    "answer": "What is the model's final answer? Give the answer only.",
}

CLI_ALIASES = {
    "correct": "correctness",
    "termination": "reasoning_term",
}


def compute_layers(model_name, n_layers=None, layers=None):
    if layers:
        return [int(layer) for layer in layers]
    n = n_layers or 3
    percents = [int(100 * (i + 1) / (n + 1)) for i in range(n)]
    return [layer_percent_to_layer(model_name, p) for p in percents]


def get_host_fqdn():
    return subprocess.check_output(["hostname", "-f"], text=True).strip()


def is_ucl_host(host_fqdn):
    return host_fqdn.endswith("ucl.ac.uk")


def get_share_target_host(bind_host):
    if bind_host in ("127.0.0.1", "0.0.0.0", "localhost"):
        return "127.0.0.1"
    return bind_host


def start_cloudflared_tunnel(target_host, target_port, timeout_s=20):
    if shutil.which("cloudflared") is None:
        raise FileNotFoundError("cloudflared is required for public sharing but is not installed")
    cmd = ["cloudflared", "tunnel", "--url", f"http://{target_host}:{target_port}", "--no-autoupdate"]
    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)
    public_url = {"value": None}

    def _reader():
        for line in proc.stdout:
            match = TRYCLOUDFLARE_URL_RE.search(line)
            if match and public_url["value"] is None:
                public_url["value"] = match.group(0)
                print(f"Public URL: {public_url['value']}")

    thread = threading.Thread(target=_reader, daemon=True)
    thread.start()
    deadline = time.time() + timeout_s
    while time.time() < deadline and proc.poll() is None and public_url["value"] is None:
        time.sleep(0.1)
    if public_url["value"] is None:
        proc.terminate()
        raise RuntimeError("cloudflared did not produce a public URL")
    atexit.register(lambda: proc.poll() is None and proc.terminate())
    return proc, public_url["value"]


def resolve_share_info(args):
    host_fqdn = get_host_fqdn()
    ucl_host = is_ucl_host(host_fqdn)
    info = {"host_fqdn": host_fqdn, "is_ucl_host": ucl_host, "share_policy": args.share_policy, "public_url": None, "tunnel_process": None}
    return info


def maybe_start_public_share(args, share_info):
    should_share = args.share_policy == "always" or (args.share_policy == "auto" and not share_info["is_ucl_host"])
    if not should_share:
        if args.share_policy == "auto" and share_info["is_ucl_host"]:
            print(f"Detected UCL host ({share_info['host_fqdn']}); skipping public URL.")
        return
    share_info["tunnel_process"], share_info["public_url"] = start_cloudflared_tunnel(get_share_target_host(args.host), args.port)


def load_train_task_config(config_path):
    with open(config_path) as f:
        config = yaml.safe_load(f)
    prompt_map = dict(BUILTIN_TASK_PROMPTS)
    task_options = [
        {"key": "custom", "label": "Custom prompt", "prompt": "", "description": "Write your own prompt."},
        {"key": "recon", "label": "Quick: recon", "prompt": prompt_map["recon"], "description": "Reconstruct the CoT."},
        {"key": "next", "label": "Quick: next", "prompt": prompt_map["next"], "description": "Predict the next ~50 tokens."},
        {"key": "answer", "label": "Quick: answer", "prompt": prompt_map["answer"], "description": "Predict the final answer."},
        {"key": "correctness", "label": "Quick: correctness", "prompt": prompt_map["correctness"], "description": "Judge whether the answer is correct."},
    ]
    seen = {item["key"] for item in task_options}
    for task_name, task_cfg in config["tasks"].items():
        enabled = task_cfg["n"] > 0 or task_cfg.get("eval_n", 0) > 0
        if not enabled:
            continue
        prompt = BUILTIN_TASK_PROMPTS.get(task_name, task_cfg["description"])
        prompt_map[task_name] = prompt
        if task_name in seen:
            continue
        task_options.append({
            "key": task_name,
            "label": f"train.yaml: {task_name}",
            "prompt": prompt,
            "description": task_cfg["description"],
        })
        seen.add(task_name)
    eval_tags = [{"key": name, "group": "eval.evals"} for name in config.get("eval", {}).get("evals", [])]
    eval_tags.extend({"key": name, "group": "eval.classification_evals"} for name in config.get("eval", {}).get("classification_evals", []))
    return prompt_map, task_options, eval_tags


def load_dual_model(model_name, checkpoint_path, cot_adapter=None, device="cuda"):
    dtype = torch.bfloat16
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.padding_side = "left"
    if not tokenizer.pad_token_id:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    kwargs = {"device_map": "auto", "torch_dtype": dtype, "attn_implementation": choose_attn_implementation(model_name)}
    use_auto_8bit = device.startswith("cuda") and torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory < AUTO_8BIT_MEMORY_THRESHOLD
    if use_auto_8bit:
        total_gb = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3
        print(f"GPU has {total_gb:.1f} GiB total memory; enabling 8-bit base-model loading.")
        kwargs["quantization_config"] = BitsAndBytesConfig(load_in_8bit=True, llm_int8_enable_fp32_cpu_offload=True)
        kwargs.pop("torch_dtype")
    print(f"Loading {model_name}...")
    model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs)
    model.eval()
    print(f"Loading trained LoRA from {checkpoint_path}...")
    model = PeftModel.from_pretrained(model, checkpoint_path, adapter_name="trained", is_trainable=False)
    ao_path = AO_CHECKPOINTS[model_name]
    print(f"Loading original AO from {ao_path}...")
    model.load_adapter(ao_path, adapter_name="original_ao", is_trainable=False)
    if cot_adapter:
        print(f"Loading model organism LoRA from {cot_adapter}...")
        model.load_adapter(cot_adapter, adapter_name="organism", is_trainable=False)
    model.eval()
    print(f"  Adapters: {list(model.peft_config.keys())}")
    return model, tokenizer


def get_model_input_device(model):
    return model.get_input_embeddings().weight.device


def get_module_device(module):
    return next(module.parameters()).device


def generate_cot_base(model, tokenizer, question, max_new_tokens=4096, device="cuda", use_organism=False, enable_thinking=True):
    messages = [{"role": "user", "content": question}]
    formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=enable_thinking)
    inputs = tokenizer(formatted, return_tensors="pt").to(get_model_input_device(model))
    if use_organism and "organism" in model.peft_config:
        model.set_adapter("organism")
        output = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
    else:
        with model.disable_adapter():
            output = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
    return tokenizer.decode(output[0][inputs["input_ids"].shape[1]:], skip_special_tokens=False)


def collect_multilayer_activations(model, tokenizer, text, layers, positions, use_organism=False, device="cuda"):
    all_acts = []
    model.eval()
    input_device = str(get_model_input_device(model))
    for layer in layers:
        adapter_name = "organism" if use_organism and "organism" in model.peft_config else None
        if adapter_name:
            model.set_adapter(adapter_name)
        acts = collect_activations_at_positions(model, tokenizer, text, layer, positions, device=input_device, adapter_name=adapter_name)
        all_acts.append(acts)
    return torch.cat(all_acts, dim=0)


def encode_prompt_with_positions(tokenizer, full_prompt, relative_spans):
    messages = [{"role": "user", "content": full_prompt}]
    formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False)
    content_start = formatted.index(full_prompt)
    encoded = tokenizer(formatted, add_special_tokens=False, return_offsets_mapping=True)
    input_ids = encoded["input_ids"]
    offsets = encoded["offset_mapping"]
    positions = []
    for rel_start, rel_end in relative_spans:
        abs_start = content_start + rel_start
        abs_end = content_start + rel_end
        token_positions = [i for i, (tok_start, tok_end) in enumerate(offsets) if tok_start < abs_end and tok_end > abs_start]
        if len(token_positions) != 1:
            raise ValueError(f"Expected exactly one token for span {(rel_start, rel_end)}, found {token_positions}")
        positions.append(token_positions[0])
    return input_ids, positions


def query_original_ao(model, tokenizer, acts_l50, prompt, model_name, injection_layer=1, max_new_tokens=150, device="cuda"):
    dtype = torch.bfloat16
    num_positions = acts_l50.shape[0]
    act_layer = layer_percent_to_layer(model_name, 50)
    prefix = f"L{act_layer}:" + SPECIAL_TOKEN * num_positions + "\n"
    full_prompt = prefix + prompt
    label_len = len(f"L{act_layer}:")
    relative_spans = [(label_len + i * len(SPECIAL_TOKEN), label_len + (i + 1) * len(SPECIAL_TOKEN)) for i in range(num_positions)]
    input_ids, positions = encode_prompt_with_positions(tokenizer, full_prompt, relative_spans)
    input_tensor = torch.tensor([input_ids], device=get_model_input_device(model))
    attn_mask = torch.ones_like(input_tensor)
    model.set_adapter("original_ao")
    injection_submodule = get_hf_submodule(model, injection_layer, use_lora=True)
    hook_fn = get_steering_hook(vectors=acts_l50, positions=positions, device=get_module_device(injection_submodule), dtype=dtype)
    with torch.no_grad(), add_hook(injection_submodule, hook_fn):
        output = model.generate(input_ids=input_tensor, attention_mask=attn_mask, max_new_tokens=max_new_tokens, do_sample=False)
    return tokenizer.decode(output[0][len(input_ids):], skip_special_tokens=True)


def query_trained_oracle(model, tokenizer, selected_acts, prompt, selected_layers, layer_counts, injection_layer=1, max_new_tokens=150, device="cuda"):
    dtype = torch.bfloat16
    parts = [f"L{layer}:" + TRAINED_PLACEHOLDER * count for layer, count in zip(selected_layers, layer_counts)]
    prefix = " ".join(parts) + "\n"
    full_prompt = prefix + prompt
    relative_spans = []
    cursor = 0
    for idx, (layer, count) in enumerate(zip(selected_layers, layer_counts)):
        cursor += len(f"L{layer}:")
        for pos_idx in range(count):
            start = cursor + pos_idx * len(TRAINED_PLACEHOLDER)
            relative_spans.append((start, start + len(TRAINED_PLACEHOLDER)))
        cursor += count * len(TRAINED_PLACEHOLDER)
        cursor += 1
    input_ids, positions = encode_prompt_with_positions(tokenizer, full_prompt, relative_spans)
    input_tensor = torch.tensor([input_ids], device=get_model_input_device(model))
    attn_mask = torch.ones_like(input_tensor)
    model.set_adapter("trained")
    injection_submodule = get_hf_submodule(model, injection_layer, use_lora=True)
    hook_fn = get_steering_hook(vectors=selected_acts, positions=positions, device=get_module_device(injection_submodule), dtype=dtype)
    with torch.no_grad(), add_hook(injection_submodule, hook_fn):
        output = model.generate(input_ids=input_tensor, attention_mask=attn_mask, max_new_tokens=max_new_tokens, do_sample=False)
    return tokenizer.decode(output[0][len(input_ids):], skip_special_tokens=True)


def select_activation_cells(multilayer_acts, ao_acts, all_layers, n_positions_per_layer, selected_cells):
    n_layers = len(all_layers)
    d_model = multilayer_acts.shape[1]
    acts_by_layer = multilayer_acts.view(n_layers, n_positions_per_layer, d_model)
    if selected_cells is None:
        mask = torch.ones((n_layers, n_positions_per_layer), dtype=torch.bool)
    else:
        mask = torch.zeros((n_layers, n_positions_per_layer), dtype=torch.bool)
        layer_to_idx = {layer: idx for idx, layer in enumerate(all_layers)}
        for cell in selected_cells:
            mask[layer_to_idx[int(cell["layer"])]][int(cell["position"])] = True
    if not torch.any(mask):
        raise ValueError("No activation cells selected")
    selected_chunks = []
    selected_layers = []
    layer_counts = []
    selected_positions = []
    for layer_idx, layer in enumerate(all_layers):
        pos_tensor = torch.nonzero(mask[layer_idx], as_tuple=False).flatten()
        if pos_tensor.numel() == 0:
            continue
        selected_layers.append(layer)
        layer_counts.append(int(pos_tensor.numel()))
        selected_chunks.append(acts_by_layer[layer_idx][pos_tensor])
        selected_positions.extend(int(pos.item()) for pos in pos_tensor)
    unique_positions = sorted(set(selected_positions))
    selected_multilayer = torch.cat(selected_chunks, dim=0)
    selected_ao = None if ao_acts is None else ao_acts[unique_positions]
    return selected_multilayer, selected_ao, selected_layers, layer_counts, unique_positions


def token_preview(tokenizer, token_id):
    text = tokenizer.decode([token_id], skip_special_tokens=False)
    text = text.replace("\n", "\\n")
    if text.strip() == "":
        text = repr(text)[1:-1]
    if len(text) > 18:
        text = text[:15] + "..."
    return text


def decode_token_text(tokenizer, token_id):
    return tokenizer.decode([token_id], skip_special_tokens=False).replace("\n", " ")


def load_saes(layers, device):
    saes = {}
    sae_local_dir = str(PROJECT_ROOT / "downloaded_saes")
    for layer in layers:
        filename = f"saes_Qwen_Qwen3-8B_batch_top_k/resid_post_layer_{layer}/trainer_{SAE_TRAINER}/ae.pt"
        print(f"Loading SAE layer {layer}...")
        saes[layer] = load_dictionary_learning_batch_topk_sae(
            repo_id=SAE_REPO,
            filename=filename,
            model_name="Qwen/Qwen3-8B",
            device=torch.device(device),
            dtype=torch.bfloat16,
            layer=layer,
            local_dir=sae_local_dir,
        )
    return saes


def load_sae_labels(layers):
    cache_dir = Path(os.environ["CACHE_DIR"])
    labels_dir = cache_dir / "sae_features" / "trainer_2" / "trainer_2" / "labels"
    labels = {}
    for layer in layers:
        labels[layer] = json.loads((labels_dir / f"labels_layer{layer}_trainer2.json").read_text())
    return labels


def build_sae_feature_description(saes, labels, layer_to_selected_acts, layers, top_k=20):
    cpu_acts = {layer: acts.to("cpu", dtype=torch.bfloat16) for layer, acts in layer_to_selected_acts.items()}
    aggregated = sae_probe_encode_and_aggregate(saes, labels, cpu_acts, layers, top_k)
    n_positions = max((acts.shape[0] for acts in cpu_acts.values()), default=0)
    return sae_probe_format_features(aggregated, n_positions)


def query_openrouter(prompt, model, api_base=OPENROUTER_API_BASE, max_tokens=300):
    api_key = os.environ["OPENROUTER_API_KEY"]
    client = openai.OpenAI(base_url=api_base, api_key=api_key)
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=max_tokens,
        temperature=0.0,
    )
    return response.choices[0].message.content or ""


def query_sae_llm(prompt, model=OPENROUTER_SAE_MODEL, api_base=OPENROUTER_API_BASE, max_tokens=300):
    return query_openrouter(prompt, model=model, api_base=api_base, max_tokens=max_tokens)


def build_black_box_prompt(question, cot_response, prompt):
    return (
        "You are a text-only monitor reading a model's visible chain-of-thought.\n\n"
        f"Original user question:\n{question[:2000]}\n\n"
        f"Visible chain-of-thought:\n{cot_response[:6000]}\n\n"
        f"Based only on the visible text above, answer this question about the reasoning:\n{prompt[:2000]}\n\n"
        "Give a concise answer."
    )


def query_black_box_monitor(question, cot_response, prompt, model=OPENROUTER_BLACKBOX_MODEL, max_tokens=300):
    return query_openrouter(build_black_box_prompt(question, cot_response, prompt), model=model, max_tokens=max_tokens)


def print_side_by_side(label_a, text_a, label_b, text_b, width=38):
    import textwrap
    lines_a = textwrap.wrap(text_a, width=width) or ["(empty)"]
    lines_b = textwrap.wrap(text_b, width=width) or ["(empty)"]
    print(f"  {'-' * width}  {'-' * width}")
    print(f"  {label_a:<{width}}  {label_b:<{width}}")
    print(f"  {'-' * width}  {'-' * width}")
    for i in range(max(len(lines_a), len(lines_b))):
        left = lines_a[i] if i < len(lines_a) else ""
        right = lines_b[i] if i < len(lines_b) else ""
        print(f"  {left:<{width}}  {right:<{width}}")
    print()


def fetch_suggested_question():
    offset = random.randint(0, SUGGESTED_QUESTION_MAX_OFFSET)
    params = urllib.parse.urlencode({"dataset": SUGGESTED_QUESTION_DATASET, "config": "default", "split": "train", "offset": offset, "limit": 100})
    url = f"https://datasets-server.huggingface.co/rows?{params}"
    with urllib.request.urlopen(url) as response:
        payload = json.load(response)
    row = random.choice(payload["rows"])["row"]
    return {"question": row["question"], "source_label": f"{SUGGESTED_QUESTION_DATASET} (Hugging Face)", "source_url": SUGGESTED_QUESTION_DATASET_URL}


@dataclass
class SessionState:
    question: str = ""
    cot_response: str = ""
    full_text: str = ""
    enable_thinking: bool = True
    stride_positions: list[int] = field(default_factory=list)
    stride_token_ids: list[int] = field(default_factory=list)
    token_labels: list[str] = field(default_factory=list)
    cot_token_texts: list[str] = field(default_factory=list)
    sampled_token_to_stride_index: list[int | None] = field(default_factory=list)
    multilayer_acts: torch.Tensor | None = None
    ao_acts: torch.Tensor | None = None


class ChatCompareWebApp:
    def __init__(self, args, share_info):
        self.args = args
        self.share_info = share_info
        self.layers = compute_layers(args.model, n_layers=args.n_layers, layers=args.layers)
        self.layer_50 = layer_percent_to_layer(args.model, 50)
        self.prompt_map, self.task_options, self.eval_tags = load_train_task_config(args.config)
        self.use_organism = args.cot_adapter is not None
        self.model, self.tokenizer = load_dual_model(args.model, args.checkpoint, cot_adapter=args.cot_adapter, device=args.device)
        self.model_lock = threading.Lock()
        self.saes = None
        self.sae_labels = None
        self.state = SessionState()
        self.log_dir = CHAT_COMPARE_LOG_DIR
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.session_id = time.strftime("%Y%m%d-%H%M%S")
        self.session_log = self.log_dir / f"session_{self.session_id}.jsonl"
        self.app = FastAPI(title="chat_compare")
        self._register_routes()
        self._log_event("session_start", {
            "model": args.model,
            "checkpoint": args.checkpoint,
            "cot_adapter": args.cot_adapter,
            "layers": self.layers,
            "stride": args.stride,
            "host_fqdn": share_info["host_fqdn"],
            "is_ucl_host": share_info["is_ucl_host"],
            "share_policy": share_info["share_policy"],
            "public_url": share_info["public_url"],
        })

    def _ensure_sae_loaded(self):
        if self.saes is not None:
            return
        self.saes = load_saes(self.layers, "cpu")
        self.sae_labels = load_sae_labels(self.layers)

    def _log_event(self, event_type, payload):
        record = {"ts": time.strftime("%Y-%m-%d %H:%M:%S"), "event": event_type, **payload}
        with self.session_log.open("a") as f:
            f.write(json.dumps(record) + "\n")

    def _write_run_markdown(self, payload):
        path = self.log_dir / f"run_{self.session_id}_{int(time.time() * 1000)}.md"
        lines = [
            f"# chat_compare run ({payload['task_key']})",
            "",
            f"- Time: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"- Question: {payload['question']}",
            f"- Eval tags: {', '.join(payload['eval_tags']) if payload['eval_tags'] else '(none)'}",
            f"- Baselines: {', '.join(payload['selected_baselines']) if payload['selected_baselines'] else '(none)'}",
            f"- Selected layers: {payload['selected_layers']}",
            f"- Selected positions: {payload['selected_positions']}",
            "",
            "## Prompt",
            "",
            payload["prompt"],
            "",
            "## CoT (preview)",
            "",
            payload["cot_preview"],
            "",
            "## Original AO",
            "",
            payload["ao_response"],
            "",
            "## Patchscopes",
            "",
            payload["patchscopes_response"],
            "",
            "## Black-Box Monitor",
            "",
            payload["black_box_response"],
            "",
            "## Trained Oracle",
            "",
            payload["trained_response"],
            "",
            "## SAE Baseline",
            "",
            payload["sae_response"],
            "",
            "## SAE Features",
            "",
            payload["sae_feature_desc"],
            "",
        ]
        path.write_text("\n".join(lines))
        return path

    def _generate_cot_only(self, question, enable_thinking=True):
        cot_response = generate_cot_base(self.model, self.tokenizer, question, max_new_tokens=4096, device=self.args.device, use_organism=self.use_organism, enable_thinking=enable_thinking)
        messages = [{"role": "user", "content": question}]
        formatted = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=enable_thinking)
        full_text = formatted + cot_response
        self.state = SessionState(
            question=question,
            cot_response=cot_response,
            full_text=full_text,
            enable_thinking=enable_thinking,
        )
        return {
            "question": question,
            "cot_response": cot_response,
            "cot_preview": cot_response[:3000],
        }

    def _extract_current_session(self):
        if not self.state.full_text:
            raise HTTPException(status_code=400, detail="Generate a CoT first")
        messages = [{"role": "user", "content": self.state.question}]
        formatted = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=self.state.enable_thinking)
        prompt_ids = self.tokenizer.encode(formatted, add_special_tokens=False)
        all_ids = self.tokenizer.encode(self.state.full_text, add_special_tokens=False)
        prompt_len = len(prompt_ids)
        stride_positions = get_cot_positions(prompt_len, len(all_ids), stride=self.args.stride, tokenizer=self.tokenizer, input_ids=all_ids)
        if len(stride_positions) < 1:
            raise ValueError("CoT is too short for any stride positions")
        input_device = str(get_model_input_device(self.model))
        multilayer_acts = collect_multilayer_activations(self.model, self.tokenizer, self.state.full_text, self.layers, stride_positions, use_organism=self.use_organism, device=self.args.device)
        ao_acts = collect_activations_at_positions(self.model, self.tokenizer, self.state.full_text, self.layer_50, stride_positions, device=input_device, adapter_name=None)
        stride_token_ids = [all_ids[pos] for pos in stride_positions]
        token_labels = [token_preview(self.tokenizer, token_id) for token_id in stride_token_ids]
        cot_token_ids = all_ids[prompt_len:]
        cot_token_texts = [decode_token_text(self.tokenizer, token_id) for token_id in cot_token_ids]
        sampled_token_to_stride_index = [None] * len(cot_token_ids)
        for stride_idx, full_pos in enumerate(stride_positions):
            sampled_token_to_stride_index[full_pos - prompt_len] = stride_idx
        self.state.stride_positions = list(stride_positions)
        self.state.stride_token_ids = stride_token_ids
        self.state.token_labels = token_labels
        self.state.cot_token_texts = cot_token_texts
        self.state.sampled_token_to_stride_index = sampled_token_to_stride_index
        self.state.multilayer_acts = multilayer_acts
        self.state.ao_acts = ao_acts
        self._log_event("generate", {
            "question": self.state.question,
            "cot_chars": len(self.state.cot_response),
            "stride_positions": list(stride_positions),
            "token_labels": token_labels,
        })
        return {
            "stride_positions": list(stride_positions),
            "token_labels": token_labels,
            "cot_token_texts": cot_token_texts,
            "sampled_token_to_stride_index": sampled_token_to_stride_index,
            "layers": self.layers,
            "layer_50": self.layer_50,
            "n_positions": len(stride_positions),
            "n_vectors": int(multilayer_acts.shape[0]),
        }

    def _resolve_run_context(self, task_key, custom_prompt, selected_cells):
        if self.state.multilayer_acts is None:
            raise HTTPException(status_code=400, detail="Generate a CoT first")
        resolved_key = CLI_ALIASES.get(task_key, task_key)
        prompt = custom_prompt.strip() if resolved_key == "custom" else self.prompt_map[resolved_key]
        if resolved_key == "custom" and not prompt:
            raise HTTPException(status_code=400, detail="Custom prompt is empty")
        selected_ml, selected_ao, selected_layers, layer_counts, selected_positions = select_activation_cells(
            self.state.multilayer_acts,
            self.state.ao_acts,
            self.layers,
            len(self.state.stride_positions),
            selected_cells,
        )
        active_cells = selected_cells if selected_cells is not None else [{"layer": layer, "position": pos} for layer in self.layers for pos in range(len(self.state.stride_positions))]
        return {
            "task_key": resolved_key,
            "prompt": prompt,
            "selected_ml": selected_ml,
            "selected_ao": selected_ao,
            "selected_layers": selected_layers,
            "layer_counts": layer_counts,
            "selected_positions": selected_positions,
            "active_cells": active_cells,
        }

    def _selected_acts_by_layer(self, ctx):
        acts_by_layer = self.state.multilayer_acts.view(len(self.layers), len(self.state.stride_positions), self.state.multilayer_acts.shape[1])
        layer_to_idx = {layer: idx for idx, layer in enumerate(self.layers)}
        per_layer_positions = {layer: [] for layer in self.layers}
        for cell in ctx["active_cells"]:
            per_layer_positions[int(cell["layer"])].append(int(cell["position"]))
        return {layer: acts_by_layer[layer_to_idx[layer], per_layer_positions[layer], :] for layer in self.layers if per_layer_positions[layer]}

    def _run_original_ao_component(self, ctx, max_tokens):
        ao_prompt = ctx["prompt"] if ctx["task_key"] == "custom" else "Can you predict the next 10 tokens that come after this?"
        with self.model_lock:
            ao_response = query_original_ao(self.model, self.tokenizer, ctx["selected_ao"], ao_prompt, model_name=self.args.model, max_new_tokens=max_tokens, device=self.args.device)
        return {"ao_prompt": ao_prompt, "ao_response": ao_response}

    def _run_trained_oracle_component(self, ctx, max_tokens):
        with self.model_lock:
            trained_response = query_trained_oracle(
                self.model,
                self.tokenizer,
                ctx["selected_ml"],
                ctx["prompt"],
                ctx["selected_layers"],
                ctx["layer_counts"],
                max_new_tokens=max_tokens,
                device=self.args.device,
            )
        return {"prompt": ctx["prompt"], "trained_response": trained_response}

    def _parse_patchscopes_strengths(self, payload):
        raw_strengths = payload["patchscopes_strengths"]
        return {layer: float(raw_strengths[str(layer)]) for layer in self.layers}

    def _run_patchscopes_component(self, ctx, max_tokens, steering_by_layer):
        per_layer_acts = self._selected_acts_by_layer(ctx)
        responses = []
        with self.model_lock:
            for layer in ctx["selected_layers"]:
                if layer not in per_layer_acts:
                    continue
                generated = run_patchscope_single(
                    self.model,
                    self.tokenizer,
                    per_layer_acts[layer],
                    ctx["prompt"],
                    injection_layer=1,
                    steering_coefficient=steering_by_layer[layer],
                    max_new_tokens=max_tokens,
                    device=str(get_model_input_device(self.model)),
                )
                responses.append(f"L{layer}: {generated}")
        return {"patchscopes_prompt": ctx["prompt"], "patchscopes_response": " ".join(responses), "patchscopes_strengths": steering_by_layer}

    def _run_black_box_component(self, ctx, max_tokens):
        black_box_response = query_black_box_monitor(self.state.question, self.state.cot_response, ctx["prompt"], max_tokens=max_tokens)
        return {"black_box_prompt": ctx["prompt"], "black_box_response": black_box_response}

    def _run_sae_component(self, ctx):
        self._ensure_sae_loaded()
        layer_to_selected_acts = self._selected_acts_by_layer(ctx)
        sae_feature_desc = build_sae_feature_description(self.saes, self.sae_labels, layer_to_selected_acts, self.layers)
        sae_prompt = SAE_LLM_GENERATION_PROMPT.format(feature_desc=sae_feature_desc, eval_question=f"Question: {ctx['prompt'][:2000]}")
        sae_response = query_sae_llm(sae_prompt)
        return {"sae_feature_desc": sae_feature_desc, "sae_response": sae_response}

    def _finalize_run(self, ctx, eval_tags, selected_baselines, ao_prompt, ao_response, patchscopes_response, black_box_response, trained_response, sae_feature_desc, sae_response):
        log_payload = {
            "task_key": ctx["task_key"],
            "question": self.state.question,
            "prompt": ctx["prompt"],
            "eval_tags": eval_tags,
            "selected_baselines": selected_baselines,
            "selected_layers": ctx["selected_layers"],
            "selected_positions": ctx["selected_positions"],
            "cot_preview": self.state.cot_response[:2000],
            "ao_response": ao_response,
            "patchscopes_response": patchscopes_response,
            "black_box_response": black_box_response,
            "trained_response": trained_response,
            "sae_feature_desc": sae_feature_desc,
            "sae_response": sae_response,
        }
        md_path = self._write_run_markdown(log_payload)
        self._log_event("run", {**log_payload, "log_path": str(md_path), "selected_cell_count": len(ctx["active_cells"])})
        return {
            "task_key": ctx["task_key"],
            "prompt": ctx["prompt"],
            "ao_prompt": ao_prompt,
            "selected_baselines": selected_baselines,
            "selected_layers": ctx["selected_layers"],
            "layer_counts": ctx["layer_counts"],
            "selected_positions": ctx["selected_positions"],
            "ao_response": ao_response,
            "patchscopes_response": patchscopes_response,
            "black_box_response": black_box_response,
            "trained_response": trained_response,
            "sae_feature_desc": sae_feature_desc,
            "sae_response": sae_response,
            "log_path": str(md_path),
        }

    def _run_query(self, task_key, custom_prompt, selected_cells, max_tokens, eval_tags, selected_baselines, patchscopes_strengths):
        ctx = self._resolve_run_context(task_key, custom_prompt, selected_cells)
        ao_result = {"ao_prompt": "", "ao_response": "(skipped)"}
        patchscopes_result = {"patchscopes_response": "(skipped)"}
        black_box_result = {"black_box_response": "(skipped)"}
        trained_result = {"trained_response": "(skipped)"}
        sae_result = {"sae_feature_desc": "(skipped)", "sae_response": "(skipped)"}
        if "ao" in selected_baselines:
            ao_result = self._run_original_ao_component(ctx, max_tokens)
        if "patch" in selected_baselines:
            patchscopes_result = self._run_patchscopes_component(ctx, max_tokens, patchscopes_strengths)
        if "bb" in selected_baselines:
            black_box_result = self._run_black_box_component(ctx, max_tokens)
        if "oracle" in selected_baselines:
            trained_result = self._run_trained_oracle_component(ctx, max_tokens)
        if "sae" in selected_baselines:
            sae_result = self._run_sae_component(ctx)
        return self._finalize_run(
            ctx,
            eval_tags,
            selected_baselines,
            ao_result["ao_prompt"],
            ao_result["ao_response"],
            patchscopes_result["patchscopes_response"],
            black_box_result["black_box_response"],
            trained_result["trained_response"],
            sae_result["sae_feature_desc"],
            sae_result["sae_response"],
        )

    def _register_routes(self):
        @self.app.get("/", response_class=HTMLResponse)
        async def index():
            return HTMLResponse(self._render_html())

        @self.app.get("/api/config")
        async def config():
            return {
                "layers": self.layers,
                "layer_50": self.layer_50,
                "stride": self.args.stride,
                "task_options": self.task_options,
                "eval_tags": self.eval_tags,
                "has_session": self.state.multilayer_acts is not None,
                "host_fqdn": self.share_info["host_fqdn"],
                "is_ucl_host": self.share_info["is_ucl_host"],
                "share_policy": self.share_info["share_policy"],
                "public_url": self.share_info["public_url"],
            }

        @self.app.post("/api/generate")
        async def generate(payload: dict):
            question = payload["question"].strip()
            if not question:
                raise HTTPException(status_code=400, detail="Question is empty")
            cot_payload = await asyncio.to_thread(self._generate_cot_only, question, bool(payload.get("enable_thinking", True)))
            extract_payload = await asyncio.to_thread(self._extract_current_session)
            return {**cot_payload, **extract_payload}

        @self.app.post("/api/generate_cot")
        async def generate_cot(payload: dict):
            question = payload["question"].strip()
            if not question:
                raise HTTPException(status_code=400, detail="Question is empty")
            return await asyncio.to_thread(self._generate_cot_only, question, bool(payload.get("enable_thinking", True)))

        @self.app.post("/api/extract_activations")
        async def extract_activations():
            return await asyncio.to_thread(self._extract_current_session)

        @self.app.get("/api/suggest_question")
        async def suggest_question():
            suggestion = fetch_suggested_question()
            self._log_event("suggest_question", suggestion)
            return suggestion

        @self.app.post("/api/run")
        async def run_query(payload: dict):
            task_key = payload["task_key"]
            custom_prompt = payload.get("custom_prompt", "")
            selected_cells = payload.get("selected_cells", [])
            max_tokens = int(payload.get("max_tokens", self.args.max_tokens))
            eval_tags = payload.get("eval_tags", [])
            selected_baselines = payload["selected_baselines"]
            patchscopes_strengths = self._parse_patchscopes_strengths(payload)
            return await asyncio.to_thread(self._run_query, task_key, custom_prompt, selected_cells, max_tokens, eval_tags, selected_baselines, patchscopes_strengths)

        @self.app.post("/api/run_original_ao")
        async def run_original_ao(payload: dict):
            ctx = self._resolve_run_context(payload["task_key"], payload.get("custom_prompt", ""), payload.get("selected_cells", []))
            max_tokens = int(payload.get("max_tokens", self.args.max_tokens))
            result = await asyncio.to_thread(self._run_original_ao_component, ctx, max_tokens)
            return {**result, "task_key": ctx["task_key"], "prompt": ctx["prompt"], "selected_layers": ctx["selected_layers"], "selected_positions": ctx["selected_positions"]}

        @self.app.post("/api/run_patchscopes")
        async def run_patchscopes(payload: dict):
            ctx = self._resolve_run_context(payload["task_key"], payload.get("custom_prompt", ""), payload.get("selected_cells", []))
            max_tokens = int(payload.get("max_tokens", self.args.max_tokens))
            patchscopes_strengths = self._parse_patchscopes_strengths(payload)
            result = await asyncio.to_thread(self._run_patchscopes_component, ctx, max_tokens, patchscopes_strengths)
            return {**result, "task_key": ctx["task_key"], "selected_layers": ctx["selected_layers"], "selected_positions": ctx["selected_positions"]}

        @self.app.post("/api/run_trained_oracle")
        async def run_trained_oracle(payload: dict):
            ctx = self._resolve_run_context(payload["task_key"], payload.get("custom_prompt", ""), payload.get("selected_cells", []))
            max_tokens = int(payload.get("max_tokens", self.args.max_tokens))
            result = await asyncio.to_thread(self._run_trained_oracle_component, ctx, max_tokens)
            return {**result, "task_key": ctx["task_key"], "selected_layers": ctx["selected_layers"], "selected_positions": ctx["selected_positions"]}

        @self.app.post("/api/run_black_box_monitor")
        async def run_black_box_monitor(payload: dict):
            ctx = self._resolve_run_context(payload["task_key"], payload.get("custom_prompt", ""), payload.get("selected_cells", []))
            max_tokens = int(payload.get("max_tokens", self.args.max_tokens))
            result = await asyncio.to_thread(self._run_black_box_component, ctx, max_tokens)
            return {**result, "task_key": ctx["task_key"], "selected_layers": ctx["selected_layers"], "selected_positions": ctx["selected_positions"]}

        @self.app.post("/api/run_sae_partial")
        async def run_sae_partial(payload: dict):
            ctx = self._resolve_run_context(payload["task_key"], payload.get("custom_prompt", ""), payload.get("selected_cells", []))
            result = await asyncio.to_thread(self._run_sae_component, ctx)
            return {**result, "task_key": ctx["task_key"], "selected_layers": ctx["selected_layers"], "selected_positions": ctx["selected_positions"]}

        @self.app.post("/api/run_sae_baseline")
        async def run_sae_baseline(payload: dict):
            ctx = self._resolve_run_context(payload["task_key"], payload.get("custom_prompt", ""), payload.get("selected_cells", []))
            result = await asyncio.to_thread(self._run_sae_component, ctx)
            return {**result, "task_key": ctx["task_key"], "selected_layers": ctx["selected_layers"], "selected_positions": ctx["selected_positions"]}

        @self.app.post("/api/finalize_run")
        async def finalize_run(payload: dict):
            ctx = self._resolve_run_context(payload["task_key"], payload.get("custom_prompt", ""), payload.get("selected_cells", []))
            eval_tags = payload.get("eval_tags", [])
            return await asyncio.to_thread(
                self._finalize_run,
                ctx,
                eval_tags,
                payload["selected_baselines"],
                payload["ao_prompt"],
                payload["ao_response"],
                payload["patchscopes_response"],
                payload["black_box_response"],
                payload["trained_response"],
                payload["sae_feature_desc"],
                payload["sae_response"],
            )

    def _render_html(self):
        return """<!doctype html>
<html>
<head>
  <meta charset=\"utf-8\">
  <title>chat_compare</title>
  <style>
    body { font-family: sans-serif; margin: 0; background: #0f172a; color: #e2e8f0; }
    .page { display: grid; grid-template-columns: 360px 1fr; min-height: 100vh; }
    .sidebar, .main { padding: 16px; }
    .sidebar { border-right: 1px solid #334155; background: #111827; }
    textarea, select, input { width: 100%; box-sizing: border-box; background: #0f172a; color: #e2e8f0; border: 1px solid #475569; border-radius: 8px; padding: 8px; }
    textarea { min-height: 120px; resize: vertical; }
    button { background: #2563eb; color: white; border: 0; border-radius: 8px; padding: 10px 12px; cursor: pointer; }
    button.secondary { background: #334155; }
    .row { display: flex; gap: 8px; margin-top: 8px; }
    .row > * { flex: 1; }
    .status { margin-top: 12px; font-size: 14px; color: #93c5fd; min-height: 20px; }
    .muted { color: #94a3b8; font-size: 13px; }
    .panel { background: #111827; border: 1px solid #334155; border-radius: 12px; padding: 12px; margin-bottom: 12px; }
    .busy { display: none; margin-top: 12px; }
    .busy.active { display: block; }
    .busy-row { display: flex; align-items: center; gap: 10px; }
    .spinner { width: 14px; height: 14px; border: 2px solid #334155; border-top-color: #60a5fa; border-radius: 999px; animation: spin 0.8s linear infinite; }
    .progress-track { margin-top: 8px; width: 100%; height: 8px; background: #1e293b; border-radius: 999px; overflow: hidden; }
    .progress-bar { width: 0%; height: 100%; background: linear-gradient(90deg, #2563eb, #60a5fa); border-radius: 999px; transition: width 0.2s ease; }
    .token-wrap { border: 1px solid #334155; border-radius: 12px; background: #020617; padding: 12px; }
    .layer-block { border: 1px solid #1e293b; border-radius: 12px; padding: 10px; background: #0b1220; margin-bottom: 10px; }
    .layer-label { display: inline-block; margin-bottom: 8px; padding: 4px 10px; border-radius: 999px; background: #1e293b; font-weight: 700; font-size: 12px; }
    .token-paragraph { white-space: pre-wrap; line-height: 1.8; user-select: none; }
    .tok { border-radius: 6px; padding: 1px 2px; }
    .tok.sampled { cursor: pointer; background: rgba(51, 65, 85, 0.35); }
    .tok.sampled:hover { background: rgba(96, 165, 250, 0.22); }
    .tok.sampled.selected { background: #1d4ed8; color: #eff6ff; }
    .tok.unsampled { color: #64748b; }
    .outputs { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 12px; }
    .text-block { white-space: normal; word-break: break-word; line-height: 1.5; }
    .selection-box { position: fixed; border: 1px solid #60a5fa; background: rgba(96, 165, 250, 0.14); pointer-events: none; display: none; z-index: 50; }
    .mini-status { display: flex; align-items: center; gap: 8px; min-height: 18px; margin-bottom: 8px; color: #94a3b8; font-size: 12px; }
    .mini-dot { width: 10px; height: 10px; border: 2px solid #334155; border-top-color: #60a5fa; border-radius: 999px; animation: spin 0.8s linear infinite; display: none; }
    .mini-status.loading .mini-dot { display: inline-block; }
    .small { font-size: 12px; }
    .pill { display: inline-block; padding: 2px 8px; border-radius: 999px; background: #1e293b; margin-right: 6px; margin-bottom: 4px; font-size: 12px; }
    .check-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 6px 10px; margin-top: 8px; }
    .inline-check { display: flex; align-items: center; gap: 8px; font-size: 12px; color: #cbd5e1; }
    .inline-check input { width: auto; margin: 0; }
    .slider-stack { display: grid; gap: 8px; margin-top: 8px; }
    .slider-row { display: grid; grid-template-columns: 40px 1fr 42px; gap: 8px; align-items: center; }
    .slider-row input { padding: 0; }
    @keyframes spin { to { transform: rotate(360deg); } }
  </style>
</head>
<body>
  <div class=\"page\">
    <div class=\"sidebar\">
      <h2 style=\"margin-top:0\">chat_compare</h2>
      <div class=\"muted\">Bind this app with <code>--host 0.0.0.0</code> to reach it off-machine; public internet access still depends on your firewall / tunnel setup.</div>
      <div class=\"small muted\" id=\"shareInfo\" style=\"margin-top:8px\"></div>
      <label style=\"display:block;margin-top:16px\">Question</label>
      <textarea id=\"question\" placeholder=\"Ask the base model to think...\"></textarea>
      <label class=\"small\" style=\"display:flex;align-items:center;gap:8px;margin-top:8px\"><input id=\"thinkingToggle\" type=\"checkbox\" checked style=\"width:auto\">Enable thinking for base-model CoT generation</label>
      <div class=\"row\"><button id=\"refreshPromptBtn\" class=\"secondary\">Refresh sample prompt</button></div>
      <div class=\"small muted\" id=\"questionSource\" style=\"margin-top:8px\"></div>
      <div class=\"row\"><button id=\"generateBtn\">Generate CoT + Activations</button></div>
      <div class=\"status\" id=\"status\"></div>
      <div class=\"busy\" id=\"busyWrap\">
        <div class=\"busy-row\"><div class=\"spinner\"></div><div class=\"small\" id=\"busyLabel\">Working...</div></div>
        <div class=\"progress-track\"><div class=\"progress-bar\"></div></div>
      </div>
      <div class=\"panel\">
        <label>Task preset (from built-ins + enabled train.yaml tasks)</label>
        <select id=\"taskSelect\"></select>
        <label style=\"display:block;margin-top:10px\">Custom prompt</label>
        <textarea id=\"customPrompt\" placeholder=\"Used when Task preset = Custom prompt\"></textarea>
        <label style=\"display:block;margin-top:10px\">train.yaml eval tags (logged only)</label>
        <select id=\"evalTags\" multiple size=\"8\"></select>
        <label style=\"display:block;margin-top:10px\">Max new tokens</label>
        <input id=\"maxTokens\" type=\"number\" value=\"150\" min=\"1\" step=\"1\">
        <label style=\"display:block;margin-top:10px\">Baselines to run</label>
        <div class=\"check-grid\">
          <label class=\"inline-check\"><input id=\"baselineAo\" type=\"checkbox\" checked>Original AO</label>
          <label class=\"inline-check\"><input id=\"baselinePatch\" type=\"checkbox\" checked>Patchscopes</label>
          <label class=\"inline-check\"><input id=\"baselineBb\" type=\"checkbox\" checked>Black-box</label>
          <label class=\"inline-check\"><input id=\"baselineOracle\" type=\"checkbox\" checked>Trained oracle</label>
          <label class=\"inline-check\"><input id=\"baselineSae\" type=\"checkbox\" checked>SAE -> LLM</label>
        </div>
        <label style=\"display:block;margin-top:10px\">Patchscopes per-layer injection strength</label>
        <div id=\"patchStrengthRows\" class=\"slider-stack\"></div>
        <div class=\"row\">
          <button id=\"runBtn\">Run selected activations</button>
          <button id=\"selectAllBtn\" class=\"secondary\">Select all</button>
          <button id=\"clearBtn\" class=\"secondary\">Clear</button>
        </div>
        <div class=\"small muted\" id=\"selectionInfo\" style=\"margin-top:8px\">No session yet.</div>
      </div>
    </div>
    <div class=\"main\">
      <div class=\"panel\">
        <div id=\"meta\" class=\"muted\">Generate a CoT to populate the selectable activation text.</div>
        <div id=\"tagPills\" style=\"margin-top:8px\"></div>
      </div>
      <div class=\"panel\">
        <h3 style=\"margin-top:0\">CoT Preview</h3>
        <div id=\"cotPreview\" class=\"text-block\"></div>
      </div>
      <div class=\"panel\">
        <h3 style=\"margin-top:0\">Layer-Replicated CoT</h3>
        <div class=\"muted\">The same CoT is repeated once per layer. Drag across highlighted stride tokens to choose exactly which activations to inject.</div>
        <div id=\"tokenRowsWrap\" class=\"token-wrap\" style=\"margin-top:12px\"></div>
      </div>
      <div class=\"outputs\">
        <div class=\"panel\">
          <h3 style=\"margin-top:0\">Original AO</h3>
          <div class=\"mini-status\" id=\"aoStatus\"><span class=\"mini-dot\"></span><span id=\"aoStatusText\"></span></div>
          <div class=\"muted small\" id=\"aoPrompt\"></div>
          <div id=\"aoOut\" class=\"text-block\"></div>
        </div>
        <div class=\"panel\">
          <h3 style=\"margin-top:0\">Patchscopes</h3>
          <div class=\"mini-status\" id=\"patchStatus\"><span class=\"mini-dot\"></span><span id=\"patchStatusText\"></span></div>
          <div class=\"muted small\" id=\"patchPrompt\"></div>
          <div id=\"patchOut\" class=\"text-block\"></div>
        </div>
        <div class=\"panel\">
          <h3 style=\"margin-top:0\">Black-Box Monitor</h3>
          <div class=\"mini-status\" id=\"bbStatus\"><span class=\"mini-dot\"></span><span id=\"bbStatusText\"></span></div>
          <div class=\"muted small\" id=\"bbPrompt\"></div>
          <div id=\"bbOut\" class=\"text-block\"></div>
        </div>
        <div class=\"panel\">
          <h3 style=\"margin-top:0\">Trained Oracle</h3>
          <div class=\"mini-status\" id=\"oracleStatus\"><span class=\"mini-dot\"></span><span id=\"oracleStatusText\"></span></div>
          <div class=\"muted small\" id=\"oraclePrompt\"></div>
          <div id=\"oracleOut\" class=\"text-block\"></div>
        </div>
        <div class=\"panel\">
          <h3 style=\"margin-top:0\">SAE -> LLM Baseline</h3>
          <div class=\"mini-status\" id=\"saeStatus\"><span class=\"mini-dot\"></span><span id=\"saeStatusText\"></span></div>
          <div class=\"muted small\">Gemini via OpenRouter answers the oracle prompt using only SAE feature descriptions</div>
          <div id=\"saeOut\" class=\"text-block\"></div>
        </div>
      </div>
      <div class=\"panel\">
        <div class=\"muted small\" id=\"logPath\"></div>
      </div>
    </div>
  </div>
  <div id=\"selectionBox\" class=\"selection-box\"></div>
  <script>
    let config = null;
    let session = null;
    let selected = new Set();
    let dragMode = null;
    let dragStart = null;
    let patchRefreshTimer = null;
    let patchRefreshNonce = 0;
    const statusEl = document.getElementById('status');
    const selectionInfo = document.getElementById('selectionInfo');
    const busyWrap = document.getElementById('busyWrap');
    const busyLabel = document.getElementById('busyLabel');
    const progressBar = document.querySelector('.progress-bar');
    const questionSource = document.getElementById('questionSource');
    const shareInfo = document.getElementById('shareInfo');
    const selectionBox = document.getElementById('selectionBox');
    const baselineInputs = {
      ao: document.getElementById('baselineAo'),
      patch: document.getElementById('baselinePatch'),
      bb: document.getElementById('baselineBb'),
      oracle: document.getElementById('baselineOracle'),
      sae: document.getElementById('baselineSae'),
    };
    const panelStatus = {
      ao: { box: document.getElementById('aoStatus'), text: document.getElementById('aoStatusText') },
      patch: { box: document.getElementById('patchStatus'), text: document.getElementById('patchStatusText') },
      bb: { box: document.getElementById('bbStatus'), text: document.getElementById('bbStatusText') },
      oracle: { box: document.getElementById('oracleStatus'), text: document.getElementById('oracleStatusText') },
      sae: { box: document.getElementById('saeStatus'), text: document.getElementById('saeStatusText') },
    };

    function setStatus(msg) { statusEl.textContent = msg; }
    function compactText(text) { return (text || '').replace(/\\s+/g, ' ').trim(); }
    function selectedBaselines() { return Object.entries(baselineInputs).filter(([, input]) => input.checked).map(([name]) => name); }
    function patchscopesStrengths() {
      const strengths = {};
      document.querySelectorAll('.patch-layer-strength').forEach(input => { strengths[input.dataset.layer] = Number(input.value); });
      return strengths;
    }
    function setPatchStrengthLabel(layer, value) {
      document.getElementById(`patchStrengthValue_${layer}`).textContent = Number(value).toFixed(1);
    }
    function renderPatchStrengthRows() {
      const wrap = document.getElementById('patchStrengthRows');
      wrap.innerHTML = '';
      config.layers.forEach(layer => {
        const row = document.createElement('div');
        row.className = 'slider-row';
        const label = document.createElement('div');
        label.textContent = `L${layer}`;
        const input = document.createElement('input');
        input.type = 'range';
        input.min = '0.0';
        input.max = '3.0';
        input.step = '0.1';
        input.value = '1.0';
        input.className = 'patch-layer-strength';
        input.dataset.layer = String(layer);
        input.addEventListener('input', () => {
          setPatchStrengthLabel(layer, input.value);
          queuePatchscopesRefresh();
        });
        const value = document.createElement('div');
        value.id = `patchStrengthValue_${layer}`;
        value.className = 'small muted';
        value.textContent = '1.0';
        row.appendChild(label);
        row.appendChild(input);
        row.appendChild(value);
        wrap.appendChild(row);
      });
    }
    async function postJson(url, payload) {
      const response = await fetch(url, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(payload) });
      const data = await response.json();
      if (!response.ok) throw new Error(data.detail || `${url} failed`);
      return data;
    }
    function setPanelLoading(name, isLoading, msg) {
      const panel = panelStatus[name];
      panel.box.classList.toggle('loading', isLoading);
      panel.text.textContent = msg || '';
    }
    function resetPanelStatuses() {
      setPanelLoading('ao', false, '');
      setPanelLoading('patch', false, '');
      setPanelLoading('bb', false, '');
      setPanelLoading('oracle', false, '');
      setPanelLoading('sae', false, '');
    }
    function setBusy(isBusy, msg, progress) {
      busyWrap.classList.toggle('active', isBusy);
      busyLabel.textContent = msg || 'Working...';
      progressBar.style.width = isBusy ? `${progress || 0}%` : '0%';
      document.getElementById('generateBtn').disabled = isBusy;
      document.getElementById('runBtn').disabled = isBusy;
      document.getElementById('selectAllBtn').disabled = isBusy;
      document.getElementById('clearBtn').disabled = isBusy;
      document.getElementById('refreshPromptBtn').disabled = isBusy;
    }
    function keyFor(layer, position) { return `${layer}:${position}`; }
    function currentRunPayload() {
      return {
        task_key: document.getElementById('taskSelect').value,
        custom_prompt: document.getElementById('customPrompt').value,
        selected_cells: selectedCells(),
        max_tokens: Number(document.getElementById('maxTokens').value),
        eval_tags: Array.from(document.getElementById('evalTags').selectedOptions).map(opt => opt.value),
        selected_baselines: selectedBaselines(),
        patchscopes_strengths: patchscopesStrengths(),
      };
    }
    function selectedCells() {
      return Array.from(selected).map(item => {
        const [layer, position] = item.split(':').map(Number);
        return { layer, position };
      });
    }
    function updateSelectionInfo() {
      if (!session) { selectionInfo.textContent = 'No session yet.'; return; }
      const total = session.layers.length * session.n_positions;
      const count = selected.size;
      selectionInfo.textContent = `${count} / ${total} cells selected (${count === 0 ? 'oracle run will fail' : 'drag to edit'})`;
    }
    function renderTaskOptions() {
      const taskSelect = document.getElementById('taskSelect');
      taskSelect.innerHTML = '';
      for (const option of config.task_options) {
        const el = document.createElement('option');
        el.value = option.key;
        el.textContent = option.label;
        taskSelect.appendChild(el);
      }
      taskSelect.addEventListener('change', () => {
        const item = config.task_options.find(opt => opt.key === taskSelect.value);
        document.getElementById('customPrompt').value = item && item.key !== 'custom' ? item.prompt : '';
      });
      taskSelect.dispatchEvent(new Event('change'));
    }
    function renderEvalTags() {
      const evalTags = document.getElementById('evalTags');
      evalTags.innerHTML = '';
      for (const tag of config.eval_tags) {
        const el = document.createElement('option');
        el.value = tag.key;
        el.textContent = `${tag.group}: ${tag.key}`;
        evalTags.appendChild(el);
      }
    }
    function renderTokenRows() {
      const wrap = document.getElementById('tokenRowsWrap');
      if (!session) { wrap.innerHTML = ''; return; }
      wrap.innerHTML = '';
      session.layers.forEach(layer => {
        const block = document.createElement('div');
        block.className = 'layer-block';
        const label = document.createElement('div');
        label.className = 'layer-label';
        label.textContent = `Layer ${layer}`;
        block.appendChild(label);
        const paragraph = document.createElement('div');
        paragraph.className = 'token-paragraph';
        session.cot_token_texts.forEach((tokenText, tokenIndex) => {
          const strideIndex = session.sampled_token_to_stride_index[tokenIndex];
          const span = document.createElement('span');
          span.className = `tok ${strideIndex === null ? 'unsampled' : 'sampled'}`;
          span.textContent = tokenText;
          if (strideIndex !== null) {
            span.dataset.layer = layer;
            span.dataset.position = strideIndex;
            span.title = `Layer ${layer}, stride token ${strideIndex}, model token ${session.stride_positions[strideIndex]}`;
            applyCellSelection(span);
            span.addEventListener('mousedown', event => {
              event.preventDefault();
              const key = keyFor(layer, strideIndex);
              dragMode = selected.has(key) ? 'remove' : 'add';
              dragStart = { x: event.clientX, y: event.clientY };
              updateSelectionBox(event.clientX, event.clientY);
              applyRectSelection();
            });
          }
          paragraph.appendChild(span);
        });
        block.appendChild(paragraph);
        wrap.appendChild(block);
      });
      updateSelectionInfo();
    }
    function applyCellSelection(cell) {
      const key = keyFor(Number(cell.dataset.layer), Number(cell.dataset.position));
      cell.classList.toggle('selected', selected.has(key));
    }
    function refreshCells() {
      document.querySelectorAll('.tok.sampled').forEach(applyCellSelection);
      updateSelectionInfo();
    }
    function setCellSelection(layer, position, isSelected) {
      const key = keyFor(layer, position);
      if (isSelected) selected.add(key); else selected.delete(key);
      refreshCells();
    }
    function updateSelectionBox(x, y) {
      if (!dragStart) return;
      const left = Math.min(dragStart.x, x);
      const top = Math.min(dragStart.y, y);
      const width = Math.abs(dragStart.x - x);
      const height = Math.abs(dragStart.y - y);
      selectionBox.style.display = 'block';
      selectionBox.style.left = `${left}px`;
      selectionBox.style.top = `${top}px`;
      selectionBox.style.width = `${width}px`;
      selectionBox.style.height = `${height}px`;
    }
    function applyRectSelection() {
      if (!dragStart || !dragMode) return;
      const box = selectionBox.getBoundingClientRect();
      document.querySelectorAll('.tok.sampled').forEach(span => {
        const rect = span.getBoundingClientRect();
        const overlaps = !(rect.right < box.left || rect.left > box.right || rect.bottom < box.top || rect.top > box.bottom);
        if (!overlaps) return;
        const layer = Number(span.dataset.layer);
        const position = Number(span.dataset.position);
        const key = keyFor(layer, position);
        if (dragMode === 'add') selected.add(key); else selected.delete(key);
      });
      refreshCells();
    }
    function endRectSelection() {
      dragMode = null;
      dragStart = null;
      selectionBox.style.display = 'none';
      selectionBox.style.width = '0px';
      selectionBox.style.height = '0px';
    }
    function selectAll() {
      if (!session) return;
      selected = new Set();
      for (const layer of session.layers) for (let pos = 0; pos < session.n_positions; pos += 1) selected.add(keyFor(layer, pos));
      refreshCells();
    }
    function clearSelection() { selected = new Set(); refreshCells(); }
    function queuePatchscopesRefresh() {
      if (patchRefreshTimer) clearTimeout(patchRefreshTimer);
      patchRefreshTimer = setTimeout(refreshPatchscopesFromSliders, 150);
    }
    async function refreshPatchscopesFromSliders() {
      patchRefreshTimer = null;
      if (!session || !baselineInputs.patch.checked || !selected.size) return;
      const nonce = patchRefreshNonce + 1;
      patchRefreshNonce = nonce;
      setPanelLoading('patch', true, 'Refreshing...');
      try {
        const data = await postJson('/api/run_patchscopes', currentRunPayload());
        if (nonce !== patchRefreshNonce) return;
        document.getElementById('patchPrompt').textContent = `Patchscopes prompt: ${data.patchscopes_prompt}`;
        document.getElementById('patchOut').textContent = compactText(data.patchscopes_response);
        setPanelLoading('patch', false, 'Updated');
      } catch (error) {
        if (nonce !== patchRefreshNonce) return;
        setPanelLoading('patch', false, 'Failed');
        setStatus(error.message);
      }
    }
    async function loadConfig() {
      const response = await fetch('/api/config');
      config = await response.json();
      if (config.public_url) {
        shareInfo.innerHTML = `Public URL: <a href="${config.public_url}" target="_blank" rel="noreferrer" style="color:#93c5fd">${config.public_url}</a>`;
      } else if (config.share_policy === 'auto' && config.is_ucl_host) {
        shareInfo.textContent = `Host ${config.host_fqdn} is classified as UCL; public sharing is disabled in auto mode.`;
      } else {
        shareInfo.textContent = `Host: ${config.host_fqdn} | share policy: ${config.share_policy}`;
      }
      renderPatchStrengthRows();
      renderTaskOptions();
      renderEvalTags();
      await loadSuggestedQuestion();
    }
    async function loadSuggestedQuestion() {
      setStatus('Fetching a sample prompt from Hugging Face...');
      const response = await fetch('/api/suggest_question');
      const data = await response.json();
      if (!response.ok) { setStatus(data.detail || 'Failed to fetch sample prompt'); return; }
      document.getElementById('question').value = data.question;
      questionSource.innerHTML = `Sample prompt source: <a href="${data.source_url}" target="_blank" rel="noreferrer" style="color:#93c5fd">${data.source_label}</a>`;
      setStatus('Loaded a sample prompt. You can refresh it for another one.');
    }
    async function generateSession() {
      const question = document.getElementById('question').value.trim();
      const enableThinking = document.getElementById('thinkingToggle').checked;
      if (!question) { setStatus('Question is empty'); return; }
      setStatus('Generating CoT...');
      setBusy(true, 'Stage 1/2: generating CoT...', 20);
      const cotResponse = await fetch('/api/generate_cot', { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ question, enable_thinking: enableThinking }) });
      const cotData = await cotResponse.json();
      if (!cotResponse.ok) { setBusy(false); setStatus(cotData.detail || 'CoT generation failed'); return; }
      document.getElementById('cotPreview').textContent = compactText(cotData.cot_preview);
      setStatus('Extracting activations...');
      setBusy(true, 'Stage 2/2: extracting activations...', 70);
      const extractResponse = await fetch('/api/extract_activations', { method: 'POST' });
      const extractData = await extractResponse.json();
      setBusy(false);
      if (!extractResponse.ok) { setStatus(extractData.detail || 'Activation extraction failed'); return; }
      session = { ...cotData, ...extractData };
      document.getElementById('cotPreview').textContent = compactText(session.cot_preview);
      document.getElementById('meta').textContent = `Question ready | ${session.n_positions} stride positions x ${session.layers.length} layers = ${session.n_vectors} vectors | AO layer ${session.layer_50}`;
      document.getElementById('aoOut').textContent = '';
      document.getElementById('patchOut').textContent = '';
      document.getElementById('bbOut').textContent = '';
      document.getElementById('oracleOut').textContent = '';
      document.getElementById('saeOut').textContent = '';
      document.getElementById('aoPrompt').textContent = '';
      document.getElementById('patchPrompt').textContent = '';
      document.getElementById('bbPrompt').textContent = '';
      document.getElementById('oraclePrompt').textContent = '';
      document.getElementById('logPath').textContent = '';
      resetPanelStatuses();
      const tagPills = document.getElementById('tagPills');
      tagPills.innerHTML = session.layers.map(layer => `<span class=\"pill\">Layer ${layer}</span>`).join('');
      selectAll();
      renderTokenRows();
      setStatus('Ready. Drag-select activations, pick a task, then run.');
    }
    async function runSelection() {
      if (!session) { setStatus('Generate a CoT first'); return; }
      const payload = currentRunPayload();
      const baselines = payload.selected_baselines;
      if (!baselines.length) { setStatus('Select at least one baseline'); return; }
      setStatus('Running baselines...');
      setBusy(true, 'Running baselines and populating panels...', 25);
      resetPanelStatuses();
      document.getElementById('aoOut').textContent = '';
      document.getElementById('patchOut').textContent = '';
      document.getElementById('bbOut').textContent = '';
      document.getElementById('oracleOut').textContent = '';
      document.getElementById('saeOut').textContent = '';
      document.getElementById('aoPrompt').textContent = '';
      document.getElementById('patchPrompt').textContent = '';
      document.getElementById('bbPrompt').textContent = '';
      document.getElementById('oraclePrompt').textContent = '';
      document.getElementById('logPath').textContent = '';
      let completed = 0;
      function advanceProgress(msg) {
        completed += 1;
        setBusy(true, msg, 25 + Math.round((completed / baselines.length) * 55));
      }
      const requests = [];
      if (baselines.includes('ao')) {
        setPanelLoading('ao', true, 'Running...');
        requests.push(postJson('/api/run_original_ao', payload).then(data => {
        document.getElementById('aoPrompt').textContent = `AO prompt: ${data.ao_prompt}`;
        document.getElementById('aoOut').textContent = compactText(data.ao_response);
        setPanelLoading('ao', false, 'Loaded');
        advanceProgress('Original AO loaded...');
        return ['ao', data];
      }).catch(error => {
        setPanelLoading('ao', false, 'Failed');
        throw error;
      }));
      } else {
        setPanelLoading('ao', false, 'Skipped');
        document.getElementById('aoOut').textContent = '(skipped)';
      }
      if (baselines.includes('patch')) {
        setPanelLoading('patch', true, 'Running...');
        requests.push(postJson('/api/run_patchscopes', payload).then(data => {
        document.getElementById('patchPrompt').textContent = `Patchscopes prompt: ${data.patchscopes_prompt}`;
        document.getElementById('patchOut').textContent = compactText(data.patchscopes_response);
        setPanelLoading('patch', false, 'Loaded');
        advanceProgress('Patchscopes loaded...');
        return ['patch', data];
      }).catch(error => {
        setPanelLoading('patch', false, 'Failed');
        throw error;
      }));
      } else {
        setPanelLoading('patch', false, 'Skipped');
        document.getElementById('patchOut').textContent = '(skipped)';
      }
      if (baselines.includes('bb')) {
        setPanelLoading('bb', true, 'Running...');
        requests.push(postJson('/api/run_black_box_monitor', payload).then(data => {
        document.getElementById('bbPrompt').textContent = `Black-box prompt: ${data.black_box_prompt}`;
        document.getElementById('bbOut').textContent = compactText(data.black_box_response);
        setPanelLoading('bb', false, 'Loaded');
        advanceProgress('Black-box monitor loaded...');
        return ['bb', data];
      }).catch(error => {
        setPanelLoading('bb', false, 'Failed');
        throw error;
      }));
      } else {
        setPanelLoading('bb', false, 'Skipped');
        document.getElementById('bbOut').textContent = '(skipped)';
      }
      if (baselines.includes('oracle')) {
        setPanelLoading('oracle', true, 'Running...');
        requests.push(postJson('/api/run_trained_oracle', payload).then(data => {
        document.getElementById('oraclePrompt').textContent = `Oracle prompt: ${data.prompt}`;
        document.getElementById('oracleOut').textContent = compactText(data.trained_response);
        setPanelLoading('oracle', false, 'Loaded');
        advanceProgress('Trained oracle loaded...');
        return ['oracle', data];
      }).catch(error => {
        setPanelLoading('oracle', false, 'Failed');
        throw error;
      }));
      } else {
        setPanelLoading('oracle', false, 'Skipped');
        document.getElementById('oracleOut').textContent = '(skipped)';
      }
      if (baselines.includes('sae')) {
        setPanelLoading('sae', true, 'Running...');
        requests.push(postJson('/api/run_sae_partial', payload).then(data => {
        document.getElementById('saeOut').textContent = compactText(data.sae_response);
        setPanelLoading('sae', false, 'Loaded');
        advanceProgress('SAE baseline loaded...');
        return ['sae', data];
      }).catch(error => {
        setPanelLoading('sae', false, 'Failed');
        throw error;
      }));
      } else {
        setPanelLoading('sae', false, 'Skipped');
        document.getElementById('saeOut').textContent = '(skipped)';
      }
      const settled = await Promise.allSettled(requests);
      const failed = settled.find(result => result.status === 'rejected');
      if (failed) {
        setBusy(false);
        setStatus(failed.reason.message);
        return;
      }
      const results = {
        ao: { ao_prompt: '', ao_response: '(skipped)' },
        patch: { patchscopes_response: '(skipped)' },
        bb: { black_box_response: '(skipped)' },
        oracle: { trained_response: '(skipped)' },
        sae: { sae_feature_desc: '(skipped)', sae_response: '(skipped)' },
      };
      settled.forEach(result => {
        const [name, data] = result.value;
        results[name] = data;
      });
      setBusy(true, 'Writing run log...', 90);
      const finalData = await postJson('/api/finalize_run', {
        ...payload,
        ao_prompt: results.ao.ao_prompt,
        ao_response: results.ao.ao_response,
        patchscopes_response: results.patch.patchscopes_response,
        black_box_response: results.bb.black_box_response,
        trained_response: results.oracle.trained_response,
        sae_feature_desc: results.sae.sae_feature_desc,
        sae_response: results.sae.sae_response,
      });
      setBusy(false);
      document.getElementById('logPath').textContent = `Saved run log: ${finalData.log_path}`;
      setStatus(`Done. Ran ${finalData.selected_baselines.join(', ')} | layers ${finalData.selected_layers.join(', ')} | positions ${finalData.selected_positions.join(', ')}`);
    }
    document.getElementById('generateBtn').addEventListener('click', generateSession);
    document.getElementById('refreshPromptBtn').addEventListener('click', loadSuggestedQuestion);
    document.getElementById('runBtn').addEventListener('click', runSelection);
    document.getElementById('selectAllBtn').addEventListener('click', selectAll);
    document.getElementById('clearBtn').addEventListener('click', clearSelection);
    window.addEventListener('mousemove', event => {
      if (!dragMode || !dragStart) return;
      updateSelectionBox(event.clientX, event.clientY);
      applyRectSelection();
    });
    window.addEventListener('mouseup', endRectSelection);
    loadConfig();
  </script>
</body>
</html>"""


def run_web(args):
    share_info = resolve_share_info(args)
    web_app = ChatCompareWebApp(args, share_info)
    maybe_start_public_share(args, share_info)
    if share_info["public_url"]:
        web_app._log_event("share_enabled", {"host_fqdn": share_info["host_fqdn"], "public_url": share_info["public_url"]})
    print(f"Serving chat_compare on http://{args.host}:{args.port}")
    if share_info["public_url"]:
        print(f"Public URL: {share_info['public_url']}")
    uvicorn.run(web_app.app, host=args.host, port=args.port, log_level="info")


def run_cli(args):
    layers = compute_layers(args.model, n_layers=args.n_layers, layers=args.layers)
    layer_50 = layer_percent_to_layer(args.model, 50)
    prompt_map, _, _ = load_train_task_config(args.config)
    model, tokenizer = load_dual_model(args.model, args.checkpoint, cot_adapter=args.cot_adapter, device=args.device)
    use_organism = args.cot_adapter is not None
    multilayer_acts = None
    ao_acts = None
    n_positions_per_layer = 0
    print("=" * 80)
    print("  CoT Oracle A/B Comparison (CLI fallback)")
    print("  Default mode is now web; use this only when you want the old REPL flow.")
    print("=" * 80)
    while True:
        user_input = input("\nQuestion> " if multilayer_acts is None else "\nAsk oracles> ").strip()
        if not user_input:
            continue
        if user_input.lower() in ("quit", "exit", "q"):
            break
        if user_input.lower() == "new":
            multilayer_acts = None
            ao_acts = None
            n_positions_per_layer = 0
            print("Starting fresh.")
            continue
        if multilayer_acts is None:
            cot_response = generate_cot_base(model, tokenizer, user_input, max_new_tokens=4096, device=args.device, use_organism=use_organism)
            print("\n--- Model CoT ---")
            print(cot_response[:1500])
            if len(cot_response) > 1500:
                print(f"... ({len(cot_response)} chars total)")
            print("--- End ---")
            formatted = tokenizer.apply_chat_template([{"role": "user", "content": user_input}], tokenize=False, add_generation_prompt=True, enable_thinking=True)
            full_text = formatted + cot_response
            prompt_ids = tokenizer.encode(formatted, add_special_tokens=False)
            all_ids = tokenizer.encode(full_text, add_special_tokens=False)
            stride_positions = get_cot_positions(len(prompt_ids), len(all_ids), stride=args.stride, tokenizer=tokenizer, input_ids=all_ids)
            n_positions_per_layer = len(stride_positions)
            input_device = str(get_model_input_device(model))
            multilayer_acts = collect_multilayer_activations(model, tokenizer, full_text, layers, stride_positions, use_organism=use_organism, device=args.device)
            ao_acts = collect_activations_at_positions(model, tokenizer, full_text, layer_50, stride_positions, device=input_device, adapter_name=None)
            print(f"Ready: {n_positions_per_layer} positions x {len(layers)} layers.")
            continue
        task_key = CLI_ALIASES.get(user_input.lower(), user_input.lower())
        prompt = prompt_map.get(task_key, user_input)
        selected_cells = [{"layer": layer, "position": pos} for layer in layers for pos in range(n_positions_per_layer)]
        selected_ml, selected_ao, selected_layers, layer_counts, _ = select_activation_cells(multilayer_acts, ao_acts, layers, n_positions_per_layer, selected_cells)
        ao_prompt = user_input if task_key not in prompt_map else "Can you predict the next 10 tokens that come after this?"
        resp_original = query_original_ao(model, tokenizer, selected_ao, ao_prompt, model_name=args.model, max_new_tokens=args.max_tokens, device=args.device)
        resp_trained = query_trained_oracle(model, tokenizer, selected_ml, prompt, selected_layers, layer_counts, max_new_tokens=args.max_tokens, device=args.device)
        print_side_by_side("ORIGINAL AO", resp_original, "TRAINED COT ORACLE", resp_trained)


def build_parser():
    parser = argparse.ArgumentParser(description="CoT Oracle A/B Comparison")
    parser.add_argument("--model", default="Qwen/Qwen3-8B")
    parser.add_argument("--checkpoint", required=True, help="Trained LoRA checkpoint path or HF repo")
    parser.add_argument("--cot-adapter", default=None, help="Model organism LoRA for CoT generation")
    parser.add_argument("--stride", type=int, default=5, help="Stride for activation positions")
    parser.add_argument("--n-layers", type=int, default=None, help="Number of evenly-spaced layers")
    parser.add_argument("--layers", type=int, nargs="+", default=None, help="Explicit layer indices (overrides --n-layers)")
    parser.add_argument("--device", default="cuda")
    parser.add_argument("--max-tokens", type=int, default=150)
    parser.add_argument("--config", default="configs/train.yaml", help="train config used to populate task presets")
    parser.add_argument("--cli", action="store_true", help="Use the old terminal REPL instead of the web UI")
    parser.add_argument("--host", default="127.0.0.1", help="Web host (use 0.0.0.0 for off-machine access)")
    parser.add_argument("--port", type=int, default=8000, help="Web port")
    parser.add_argument("--share-policy", choices=["never", "auto", "always"], default="auto", help="Create a public Cloudflare quick-tunnel URL: never, auto (only when hostname -f is not *.ucl.ac.uk), or always")
    return parser


def main():
    args = build_parser().parse_args()
    if args.cli:
        run_cli(args)
        return
    run_web(args)


if __name__ == "__main__":
    main()
