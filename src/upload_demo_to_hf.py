"""
Upload demo dataset to HuggingFace.
"""

import json
import os
from pathlib import Path
from huggingface_hub import HfApi, create_repo

def create_readme(output_dir: Path, num_examples: int):
    """Create a README for the dataset."""
    readme = f"""---
license: mit
task_categories:
  - text-generation
language:
  - en
tags:
  - chain-of-thought
  - reasoning
  - interpretability
  - thought-anchors
  - qwen3
size_categories:
  - n<1K
---

# Qwen3-8B MATH CoT Rollouts (Demo)

Demo dataset of Chain-of-Thought reasoning traces from Qwen3-8B on MATH Level 5 problems.

## Overview

This dataset contains {num_examples} examples of:
- **MATH problems** (Level 5 difficulty)
- **Full CoT traces** with sentence-level chunking
- **Causal analysis** generated by Gemini 2 Flash

## Data Structure

Each example contains:
- `problem_id`: Unique identifier
- `problem`: The math problem text
- `problem_type`: Category (Number Theory, Precalculus, etc.)
- `gt_answer`: Ground truth answer
- `model_answer`: Qwen3-8B's answer
- `is_correct`: Whether the model got it right
- `num_chunks`: Number of sentence-level chunks in the CoT
- `chunks`: List of reasoning chunks (sentences)
- `full_cot`: Complete chain-of-thought
- `causal_analysis`: Gemini-generated analysis of reasoning patterns

## Purpose

This dataset supports research on:
1. **Thought Anchors**: Identifying which CoT sentences have outsized causal importance
2. **Reasoning interpretability**: Understanding how LLMs actually solve math problems
3. **Activation oracles**: Training models to predict causal structure from activations

## Related Work

- [Thought Anchors](https://arxiv.org/abs/2506.19143) - Bogdan et al., 2025
- [Thought Branches](https://arxiv.org/abs/2510.27484) - Macar, Bogdan et al., 2025
- [Activation Oracles](https://arxiv.org/abs/2512.15674) - Karvonen et al., 2024

## Generation Details

- **Model**: Qwen/Qwen3-8B
- **Temperature**: 0.6
- **Top-p**: 0.95
- **Problems**: MATH Level 5

## Usage

```python
from datasets import load_dataset

ds = load_dataset("your-username/qwen3-8b-math-cot-demo")

for example in ds["train"]:
    print(f"Problem: {{example['problem'][:100]}}...")
    print(f"Answer: {{example['model_answer']}} (correct: {{example['is_correct']}})")
    print(f"Analysis: {{example['causal_analysis']}}")
```

## License

MIT
"""
    with open(output_dir / "README.md", "w") as f:
        f.write(readme)


def main(json_path: str, repo_id: str):
    """Upload dataset to HuggingFace."""
    # Load data
    with open(json_path) as f:
        data = json.load(f)

    # Create temp directory for upload
    output_dir = Path("/tmp/hf_upload")
    output_dir.mkdir(exist_ok=True)

    # Save as JSONL (standard format)
    jsonl_path = output_dir / "train.jsonl"
    with open(jsonl_path, "w") as f:
        for item in data:
            f.write(json.dumps(item) + "\n")

    # Create README
    create_readme(output_dir, len(data))

    # Upload to HuggingFace
    api = HfApi()

    # Create repo if it doesn't exist
    try:
        create_repo(repo_id, repo_type="dataset", exist_ok=True)
    except Exception as e:
        print(f"Repo creation note: {e}")

    # Upload files
    api.upload_folder(
        folder_path=str(output_dir),
        repo_id=repo_id,
        repo_type="dataset",
    )

    print(f"Uploaded to https://huggingface.co/datasets/{repo_id}")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--json_path", required=True)
    parser.add_argument("--repo_id", required=True, help="e.g., username/dataset-name")
    args = parser.parse_args()

    main(args.json_path, args.repo_id)
