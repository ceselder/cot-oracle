"""
Eval 4: Decorative CoT

Is the model's chain-of-thought load-bearing or decorative?
- Decorative: model gets it right WITH and WITHOUT CoT
- Load-bearing: model only gets it right WITH CoT

Requires running the model N times per condition (expensive).
Ground truth is determined during run_evals.py, not at dataset generation time.
"""

import random
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from evals.common import EvalItem
from evals.datasets.test_splits import load_math_test


def generate_decorative_cot_dataset(n: int = 100, seed: int = 42) -> list[EvalItem]:
    """Generate decorative CoT eval examples.

    clean_prompt and test_prompt are the same question â€” the difference
    is that clean_prompt will be run WITHOUT CoT (enable_thinking=False)
    and test_prompt will be run WITH CoT (enable_thinking=True).
    """
    random.seed(seed)

    problems = load_math_test(n * 2, seed=seed)
    items = []

    for i in range(min(n, len(problems))):
        problem = problems[i]
        items.append(EvalItem(
            eval_name="decorative_cot",
            example_id=f"decorative_{i:04d}",
            clean_prompt=problem["question"],
            test_prompt=problem["question"],  # Same question, different inference mode
            correct_answer=problem["correct_answer"],
            nudge_answer=None,
            metadata={"n_runs": 5, "source": problem["source"]},
        ))

    return items
