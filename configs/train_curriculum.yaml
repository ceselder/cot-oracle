# CoT Oracle Training Config — CURRICULUM (Sequential)
# Each task is trained one at a time in order, with phase checkpoints.
# Purpose: see per-task learning curves and eval impact in isolation.

tasks:
  full_recon:
    n: 40000
    description: "Reconstruct full CoT from stride activations"

  next_step:
    n: 30000
    description: "Predict next ~50 tokens of CoT"

  answer_pred:
    n: 20000
    description: "Predict final answer from partial CoT"

  load_bearing:
    n: 15000
    description: "Is this CoT load-bearing or decorative?"

  correctness:
    n: 15000
    description: "Did the model get the right answer?"

  decorative:
    n: 15000
    description: "Is the CoT decorative?"

  domain:
    n: 0
    description: "What domain is this problem?"

  reasoning_term:
    n: 15000
    description: "Will the model emit </think> soon?"

  conv_qa:
    n: 10000
    description: "Answer conversational questions about the CoT"

# Training
training:
  lr: 0.00001
  batch_size: 8
  eval_batch_size: 2
  epochs: 1
  warmup_fraction: 0.1
  max_grad_norm: 1.0
  steering_coefficient: 1.0
  gradient_checkpointing: true
  task_order: sequential    # CURRICULUM: train tasks one at a time with phase checkpoints
  seed: 42

# Activation extraction
activations:
  stride: 5
  n_prompt_positions: 5
  position_encoding: false
  pe_alpha: 0.1

# Eval — every 1k steps as requested
eval:
  eval_steps: 1000
  save_steps: 2000
  eval_steps_detection: 1000
  eval_items_detection: 20

# Data
data:
  corpus: data/cot_corpus_v5/corpus_medium.jsonl
  precomputed_dir: data/precomputed
  concept_corpus: data/concept_corpus/corpus_full.jsonl
  cotqa_path: data/concept_corpus/corpus_full_conv_qa_llm.jsonl

# Model
model:
  name: Qwen/Qwen3-8B
  ao_checkpoint: adamkarvonen/checkpoints_latentqa_cls_past_lens_addition_Qwen3-8B
  fresh_lora: false

# Output
output:
  save_dir: checkpoints/curriculum
  wandb_project: cot_oracle
  wandb_entity: MATS10-CS-JB
  wandb_run: null
