# CoT Oracle Training Config (Unified Task System)
# Set n: 0 to disable a task. Eval-only tasks are always evaluated.
# All data is downloaded from HuggingFace automatically.

tasks:
  # --- Training + Eval (6 tasks, 80/20 train/test split on HF) ---
  hint_admission:
    n: 15000
    description: "Did the model use an external hint? (binary, CoT activations only)"

  atypical_answer:
    n: 20000
    description: "Majority or minority answer? (from 25-rollout distributions)"

  reasoning_termination:
    n: 15000
    description: "Will the model emit </think> within ~100 tokens? (partial CoT prefix)"

  answer_trajectory:
    n: 60000
    description: "What does the model currently think the answer is? (per-sentence tracking)"

  futurelens:
    n: 30000
    description: "Predict next ~50 tokens of reasoning (single activation, 3 layers)"

  correctness:
    n: 7500
    description: "Did the model's reasoning lead to the correct answer? (binary)"

  backtrack_prediction:
    n: 15000
    description: "Will the model revise/backtrack in the next ~150 tokens?"

# FineWeb context prediction (PastLens-style auxiliary training)
# Streams FineWeb + LMSYS chat, generates past/future token prediction examples.
fineweb:
  enabled: false          # toggle on to mix web text context prediction into training
  n: 50000
  max_context_tokens: 2000

# Training
training:
  lr: 0.00001
  batch_size: 32             # per-GPU micro-batch size
  effective_batch_size: 256  # total across GPUs + accumulation (invariant to GPU count)
  eval_batch_size: 8
  epochs: 1
  warmup_fraction: 0.1
  max_grad_norm: 1.0
  steering_coefficient: 1.0
  gradient_checkpointing: true
  task_order: shuffled    # "shuffled" (default), "sequential" (one at a time), or "interleaved" (round-robin blocks)
  interleave_blocks: 50   # number of blocks for interleaved mode
  max_train_tokens_per_gpu: 32768
  seed: 42

# Flamingo cross-attention (alternative to steering injection)
flamingo:
  enabled: false
  xattn_interval: 4       # cross-attention every N transformer blocks (9 layers for Qwen3-8B)
  xattn_lora_r: 64        # LoRA rank for cross-attention projections

# Activation extraction
activations:
  stride: 5
  n_prompt_positions: 5
  position_encoding: false   # sinusoidal PE for activation vectors
  pe_alpha: 0.1              # PE mixing coefficient (only used if position_encoding: true)

# Eval (unified: all evals use HF test splits via eval_loop.py)
eval:
  eval_steps: 2000
  save_steps: 10000
  rot13_start_step: 2000
  max_items_per_eval: 25
  eval_batch_size: 4

# Model
model:
  name: Qwen/Qwen3-8B
  ao_checkpoint: adamkarvonen/checkpoints_latentqa_cls_past_lens_addition_Qwen3-8B
  fresh_lora: true

# Output
output:
  save_dir: checkpoints
  wandb_project: cot_oracle
  wandb_entity: MATS10-CS-JB
  wandb_run: null  # auto-generated if null
