# CoT Oracle Training Config (Unified Task System)
# Each task has `n` (training samples, 0 to skip) and `eval` (whether to evaluate).
# All data is downloaded from HuggingFace automatically.

tasks:
  # --- Trainable tasks (have train + test splits on HF) ---
  hint_admission:
    n: 15000
    eval: true

  atypical_answer:
    n: 20000
    eval: true

  reasoning_termination:
    n: 15000
    eval: true

  answer_trajectory:
    n: 60000
    eval: true

  futurelens:
    n: 30000
    eval: true

  correctness:
    n: 7500
    eval: true

  backtrack_prediction:
    n: 15000
    eval: true

  # --- Eval-only tasks (test split only, n is ignored) ---
  sycophancy:
    eval: true

  truthfulqa_hint_verbalized:
    eval: true

  truthfulqa_hint_unverbalized:
    eval: true

  sentence_insertion:
    eval: true

  rot13_reconstruction:
    eval: false     # skip for now

# FineWeb context prediction (PastLens-style auxiliary training)
fineweb:
  enabled: false
  n: 50000
  max_context_tokens: 2000

# Training
training:
  lr: 0.00001
  batch_size: 32
  effective_batch_size: 256
  eval_batch_size: 8
  epochs: 1
  warmup_fraction: 0.1
  max_grad_norm: 1.0
  steering_coefficient: 1.0
  gradient_checkpointing: true
  task_order: shuffled
  interleave_blocks: 50
  max_train_tokens_per_gpu: 32768
  seed: 42

# Flamingo cross-attention
flamingo:
  enabled: false
  xattn_interval: 4
  xattn_lora_r: 64

# Activation extraction
activations:
  stride: 5
  n_prompt_positions: 5
  position_encoding: false
  pe_alpha: 0.1

# Eval
eval:
  eval_steps: 2000
  save_steps: 10000
  max_items_per_eval: 25
  eval_batch_size: 4

# Model
model:
  name: Qwen/Qwen3-8B
  ao_checkpoint: adamkarvonen/checkpoints_latentqa_cls_past_lens_addition_Qwen3-8B
  fresh_lora: true

# Output
output:
  save_dir: checkpoints
  wandb_project: cot_oracle
  wandb_entity: MATS10-CS-JB
  wandb_run: null
