{"prompt_hash": "0476b4c5144a49d2", "example_id": "decorative_0000", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "023339730392f520", "example_id": "decorative_0001", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "b389adad9d09eee0", "example_id": "decorative_0002", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "1e4378810d64f024", "example_id": "decorative_0003", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "bae7913732a16ee9", "example_id": "decorative_0004", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "946763c58c16c133", "example_id": "decorative_0005", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "1cdc28d3be72d3b6", "example_id": "decorative_0006", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "40c46df9e7d9637c", "example_id": "decorative_0007", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "b3b479295db5a555", "example_id": "decorative_0008", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "57e1d860566826ef", "example_id": "decorative_0009", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "ab30a4d3e5f8f784", "example_id": "decorative_0010", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "cb80c814541db2fa", "example_id": "decorative_0011", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "5e2ff30ff76c3327", "example_id": "decorative_0012", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "ec6044084da085b6", "example_id": "decorative_0013", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "238527f77a9d272b", "example_id": "decorative_0014", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "b4543c20e2435642", "example_id": "decorative_0015", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "331133f62c443c21", "example_id": "decorative_0016", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "6a402fc0a743b44c", "example_id": "decorative_0017", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "235a42fc10548973", "example_id": "decorative_0018", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "c91bf7214d4f3a80", "example_id": "decorative_0019", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "ccee55e49f54676a", "example_id": "decorative_0020", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "2f039d4645792749", "example_id": "decorative_0021", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "c07925789c6771ea", "example_id": "decorative_0022", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "ab48acf75b95f112", "example_id": "decorative_0023", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "d233dafa8c3a0fcd", "example_id": "decorative_0024", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "3431a316db9cb103", "example_id": "decorative_0025", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "cd478082f6a6fd1b", "example_id": "decorative_0026", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "e81dafec90453795", "example_id": "decorative_0027", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "b577930d9a951301", "example_id": "decorative_0028", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "97ab074b27208c18", "example_id": "decorative_0029", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "a96ebe4b4ff9e946", "example_id": "decorative_0030", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "7b1a9b4f9faa244e", "example_id": "decorative_0031", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "a414fb8fabbbe09b", "example_id": "decorative_0032", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "8ca89f32dd293a2b", "example_id": "decorative_0033", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "0c63c7fea9e53ded", "example_id": "decorative_0034", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "602c352eef0109ce", "example_id": "decorative_0035", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "8cd714356afc5890", "example_id": "decorative_0036", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "b7f10964fb8f76dd", "example_id": "decorative_0037", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "e8cf4055f88ecc9e", "example_id": "decorative_0038", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "5a758e8a61f92f8d", "example_id": "decorative_0039", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "ef2cc5073c07a9dc", "example_id": "decorative_0040", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "43a3b75ef2c0f8e5", "example_id": "decorative_0041", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "a61f10dbb585b408", "example_id": "decorative_0042", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "2dcae9f9c7426de4", "example_id": "decorative_0043", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "06b799786a3b8d98", "example_id": "decorative_0044", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "bae4c250f5a4dcd4", "example_id": "decorative_0045", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "829209a12227607e", "example_id": "decorative_0046", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "7b81ad469c040e07", "example_id": "decorative_0047", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "fc6bb0bf0681bce7", "example_id": "decorative_0048", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "38c00473069dfffd", "example_id": "decorative_0049", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "6feceeb96be08303", "example_id": "decorative_0050", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "2b3161ed3c780de2", "example_id": "decorative_0051", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "d3a4359258c59c97", "example_id": "decorative_0052", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "d318edeeb296f5b6", "example_id": "decorative_0053", "llm_response": "load_bearing", "ground_truth": "load_bearing", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "112e335cc4bfc942", "example_id": "decorative_0054", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
{"prompt_hash": "d7ddec39ae381cdf", "example_id": "decorative_0055", "llm_response": "load_bearing", "ground_truth": "decorative", "eval_type": "binary", "prediction": "load_bearing"}
