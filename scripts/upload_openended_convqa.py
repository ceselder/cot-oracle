#!/usr/bin/env python3
"""
Upload open-ended ConvQA dataset to HuggingFace with train/test split.

Usage:
    python scripts/upload_openended_convqa.py --dry-run
    python scripts/upload_openended_convqa.py
"""

import argparse
import json
import os
import random
from collections import Counter
from pathlib import Path

import pandas as pd
from dotenv import load_dotenv
from huggingface_hub import HfApi, create_repo

load_dotenv(os.path.expanduser("~/.env"))

PROJECT_ROOT = Path(__file__).parent.parent
REPO_ID = "mats-10-sprint-cs-jb/cot-oracle-convqa"


def main():
    parser = argparse.ArgumentParser(description="Upload open-ended ConvQA to HF")
    parser.add_argument("--input", default="data/openended_qa/openended_convqa.jsonl")
    parser.add_argument("--test-fraction", type=float, default=0.1)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--dry-run", action="store_true")
    args = parser.parse_args()

    # Load JSONL
    rows = []
    with open(args.input) as f:
        for line in f:
            if line.strip():
                rows.append(json.loads(line))
    print(f"Loaded {len(rows)} rows from {args.input}")

    df = pd.DataFrame(rows)

    # Stats
    print(f"\n  By source:")
    for src, cnt in sorted(Counter(df["source"]).items()):
        print(f"    {src}: {cnt}")

    print(f"\n  CoT length (sentences):")
    print(f"    mean: {df['n_sentences'].mean():.1f}, median: {df['n_sentences'].median():.1f}, min: {df['n_sentences'].min()}, max: {df['n_sentences'].max()}")

    # Train/test split by cot_id (all rows for a CoT go to same split)
    cot_ids = sorted(df["cot_id"].unique())
    rng = random.Random(args.seed)
    rng.shuffle(cot_ids)
    test_end = int(len(cot_ids) * args.test_fraction)
    test_ids = set(cot_ids[:test_end])
    train_ids = set(cot_ids[test_end:])

    df_train = df[df["cot_id"].isin(train_ids)]
    df_test = df[df["cot_id"].isin(test_ids)]
    print(f"\n  Split: {len(df_train)} train ({len(train_ids)} CoTs), {len(df_test)} test ({len(test_ids)} CoTs)")

    # Save parquets
    output_dir = PROJECT_ROOT / "data" / "hf_uploads" / "convqa_openended"
    output_dir.mkdir(parents=True, exist_ok=True)

    train_path = output_dir / "train.parquet"
    test_path = output_dir / "test.parquet"
    df_train.to_parquet(train_path, index=False)
    df_test.to_parquet(test_path, index=False)
    print(f"  Saved {len(df_train)} train → {train_path}")
    print(f"  Saved {len(df_test)} test → {test_path}")

    # Build README
    source_dist = Counter(df["source"])
    source_table = "\n".join(f"| {src} | {cnt} |" for src, cnt in sorted(source_dist.items()))

    readme = f"""---
tags:
  - cot-oracle
  - chain-of-thought
  - reasoning-analysis
license: mit
---

# CoT Oracle: Open-Ended ConvQA

Open-ended question-answer dataset for training chain-of-thought oracles.

Each Qwen3-8B chain-of-thought is sent to Gemini 2.5 Flash Lite, which generates
an open-ended question about the reasoning (`prompt`), then answers it (`target_response`).

- **Round 1:** Gemini sees the full CoT, generates a question about the reasoning.
- **Round 2:** Gemini sees the full CoT + the question, produces an answer.

## Splits

| Split | CoTs | Rows |
|-------|------|------|
| train | {len(train_ids)} | {len(df_train)} |
| test | {len(test_ids)} | {len(df_test)} |

Split by `cot_id` (all data for a given CoT goes to the same split).

## Sources ({len(source_dist)} task domains)

| Source | Count |
|--------|-------|
{source_table}

**Total: {len(rows)} rows** (1 per CoT)

## Schema

| Field | Description |
|-------|-------------|
| `cot_text` | Full chain-of-thought reasoning text |
| `prompt` | Open-ended question generated by Gemini (Round 1) |
| `target_response` | Gemini's answer to the question (Round 2) |
| `cot_id` | Source corpus ID |
| `source` | Task domain (MATH, GSM8K, etc.) |
| `original_question` | Original problem the CoT was solving |
| `n_sentences` | Number of sentences in CoT |
| `question_generation_prompt` | System prompt template used in Round 1 (identical for all rows) |

## Alternate Question Generation Prompts

The dataset was generated using prompt **#1** below. To regenerate with a different
style, swap the `question_generation_prompt` system prompt in the generation script.

1. **(Used)** Reasoning strategy / key decision / non-obvious step:
   > You are analyzing a chain-of-thought reasoning trace for a math/logic problem.
   > Generate a single open-ended question that asks about the reasoning strategy,
   > a key decision, or a non-obvious step in this trace. The question should require
   > reading the full chain of thought to answer well — it should not be answerable
   > from the problem statement alone. Output ONLY the question, nothing else.

2. Key insight or breakthrough moment:
   > You are analyzing a chain-of-thought reasoning trace. Generate a single
   > open-ended question that asks the reader to identify the key insight,
   > breakthrough moment, or pivotal deduction in the reasoning. The question
   > should target the most important turning point. Output ONLY the question.

3. Logical soundness and error detection:
   > You are analyzing a chain-of-thought reasoning trace. Generate a single
   > open-ended question that asks the reader to evaluate whether the reasoning
   > is logically sound, and to identify any errors, gaps, or unjustified leaps
   > if present. Output ONLY the question.

4. Uncertainty and self-correction:
   > You are analyzing a chain-of-thought reasoning trace. Generate a single
   > open-ended question about moments of uncertainty, hesitation, self-correction,
   > or backtracking in the reasoning. The question should probe where the model
   > was least confident. Output ONLY the question.

5. Alternative approaches:
   > You are analyzing a chain-of-thought reasoning trace. Generate a single
   > open-ended question about what alternative approaches or methods the reasoning
   > could have taken, or how the approach would differ if a key assumption changed.
   > Output ONLY the question.

6. Conclusion tracing:
   > You are analyzing a chain-of-thought reasoning trace. Generate a single
   > open-ended question that asks the reader to explain how the final conclusion
   > or answer was reached, tracing the logical chain from premises to result.
   > Output ONLY the question.

7. Step comparison:
   > You are analyzing a chain-of-thought reasoning trace. Generate a single
   > open-ended question that asks the reader to compare or contrast different
   > steps, sub-goals, or phases within the reasoning, or to explain how early
   > steps connect to later ones. Output ONLY the question.

## Usage

```python
from datasets import load_dataset
ds = load_dataset("{REPO_ID}")
train = ds["train"]
test = ds["test"]
```
"""

    with open(output_dir / "README.md", "w") as f:
        f.write(readme)

    if not args.dry_run:
        token = os.environ["HF_TOKEN"]
        api = HfApi(token=token)
        create_repo(REPO_ID, repo_type="dataset", exist_ok=True, token=token)
        api.upload_folder(folder_path=str(output_dir), repo_id=REPO_ID, repo_type="dataset")
        print(f"  Uploaded to https://huggingface.co/datasets/{REPO_ID}")
    else:
        print(f"  Dry run — would upload to https://huggingface.co/datasets/{REPO_ID}")

    print("\nDone!")


if __name__ == "__main__":
    main()
