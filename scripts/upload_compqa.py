#!/usr/bin/env python3
"""
Format and upload CompQA eval dataset to HuggingFace.

CompQA = computational questions about CoT reasoning traces (soundness,
errors, non-sequiturs, load-bearing steps, etc.). 10 query types, ~1000
examples each, answered by Qwen3-8B via OpenRouter.

Usage:
    python scripts/upload_compqa.py --dry-run
    python scripts/upload_compqa.py
"""

import argparse
import json
import os
from pathlib import Path

import pandas as pd
from dotenv import load_dotenv
from huggingface_hub import HfApi, create_repo

load_dotenv(os.path.expanduser("~/.env"))

PROJECT_ROOT = Path(__file__).parent.parent
COLLECTION_PREFIX = "mats-10-sprint-cs-jb/cot-oracle-eval"
REPO_SUFFIX = "compqa"


def build_dataset(input_path: Path) -> list[dict]:
    """Convert CompQA JSONL into the standard eval format."""
    items = []
    n_errors = 0
    with open(input_path) as f:
        for line in f:
            entry = json.loads(line)
            if entry["response"].startswith("ERROR:"):
                n_errors += 1
                continue

            items.append({
                "eval_name": f"compqa_{entry['query_type']}",
                "example_id": f"compqa_{entry['corpus_id']}_{entry['query_type']}",
                "clean_prompt": entry["question"],
                "test_prompt": entry["query"],
                "correct_answer": entry["response"],
                "nudge_answer": None,
                "meta_query_type": entry["query_type"],
                "meta_corpus_id": entry["corpus_id"],
                "meta_cot_correct": entry["cot_correct"],
                "meta_n_sentences": entry["n_sentences"],
                "meta_model": entry["model"],
                "meta_cot_text": entry["cot_text"],
            })

    print(f"  {len(items)} items ({n_errors} errors skipped)")
    return items


def save_and_upload(items: list[dict], dry_run: bool):
    repo_id = f"{COLLECTION_PREFIX}-{REPO_SUFFIX}"

    output_dir = PROJECT_ROOT / "data" / "hf_uploads" / REPO_SUFFIX
    output_dir.mkdir(parents=True, exist_ok=True)

    parquet_path = output_dir / "train.parquet"
    df = pd.DataFrame(items)
    df.to_parquet(parquet_path, index=False)

    n_items = len(items)
    num_bytes = parquet_path.stat().st_size

    # Count per query type
    from collections import Counter
    type_dist = Counter(item["meta_query_type"] for item in items)
    type_table = "\n".join(f"| `{qt}` | {cnt} |" for qt, cnt in sorted(type_dist.items()))

    # Build features YAML
    base_features = [
        ("eval_name", "string"), ("example_id", "string"), ("clean_prompt", "string"),
        ("test_prompt", "string"), ("correct_answer", "string"), ("nudge_answer", "string"),
    ]
    meta_features = [
        ("meta_query_type", "string"), ("meta_corpus_id", "string"),
        ("meta_cot_correct", "bool"), ("meta_n_sentences", "int64"),
        ("meta_model", "string"), ("meta_cot_text", "string"),
    ]
    features_yaml = ""
    for name, dtype in base_features + meta_features:
        features_yaml += f"    - name: {name}\n      dtype: {dtype}\n"

    readme = f"""---
tags:
  - cot-oracle
  - chain-of-thought
  - reasoning-analysis
  - eval
license: mit
dataset_info:
  features:
{features_yaml}  splits:
    - name: train
      num_bytes: {num_bytes}
      num_examples: {n_items}
---

# CoT Oracle Eval: CompQA

Computational questions about chain-of-thought reasoning traces. 10 query types
probing soundness, errors, non-sequiturs, load-bearing steps, self-correction, and more.

Answers generated by **Qwen3-8B** via OpenRouter. Source CoTs from
[ceselder/qwen3-8b-math-cot-corpus](https://huggingface.co/datasets/ceselder/qwen3-8b-math-cot-corpus).

Part of the [CoT Oracle Evals collection](https://huggingface.co/collections/mats-10-sprint-cs-jb/cot-oracle-evals).

## Query Types

| Type | Count |
|------|-------|
{type_table}

**Total: {n_items} examples**

## Schema

| Field | Description |
|-------|-------------|
| `eval_name` | `"compqa_{{query_type}}"` |
| `example_id` | Unique identifier |
| `clean_prompt` | Original question (no CoT) |
| `test_prompt` | The CompQA query about the CoT |
| `correct_answer` | Qwen3-8B reference answer |
| `nudge_answer` | null (no nudge in this eval) |
| `meta_query_type` | One of 10 query types |
| `meta_corpus_id` | Source CoT identifier |
| `meta_cot_correct` | Whether the CoT reached the correct answer |
| `meta_n_sentences` | Number of sentences in the CoT |
| `meta_model` | Model that generated the answer |
| `meta_cot_text` | Full CoT text |

## Usage

```python
from datasets import load_dataset
ds = load_dataset("{repo_id}", split="train")
```

## Project

- Code: [cot-oracle](https://github.com/japhba/cot-oracle)
- Training data: [ceselder/qwen3-8b-math-cot-corpus](https://huggingface.co/datasets/ceselder/qwen3-8b-math-cot-corpus)
"""
    with open(output_dir / "README.md", "w") as f:
        f.write(readme)

    print(f"  Saved {n_items} items to {parquet_path}")

    if not dry_run:
        token = os.environ["HF_TOKEN"]
        api = HfApi(token=token)
        create_repo(repo_id, repo_type="dataset", exist_ok=True, token=token)
        api.upload_folder(folder_path=str(output_dir), repo_id=repo_id, repo_type="dataset")
        print(f"  Uploaded to https://huggingface.co/datasets/{repo_id}")
    else:
        print(f"  Dry run â€” would upload to https://huggingface.co/datasets/{repo_id}")


def main():
    parser = argparse.ArgumentParser(description="Upload CompQA eval dataset to HF")
    parser.add_argument("--input", default="data/compqa/qwen3_8b.jsonl")
    parser.add_argument("--dry-run", action="store_true")
    args = parser.parse_args()

    print("Building CompQA dataset...")
    items = build_dataset(Path(args.input))

    # Print distribution
    from collections import Counter
    dist = Counter(item["meta_query_type"] for item in items)
    print("  Query type distribution:")
    for qt, cnt in sorted(dist.items()):
        print(f"    {qt}: {cnt}")

    save_and_upload(items, args.dry_run)
    print("\nDone!")


if __name__ == "__main__":
    main()
